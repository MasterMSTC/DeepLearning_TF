{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"MSTC_Keras_RNN_2_Text_2018.ipynb","version":"0.3.2","views":{},"default_view":{},"provenance":[{"file_id":"1ossNQSrXyvkmF7ZvZr7QPryk49A9tpCX","timestamp":1521206578358}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"dUNKDytx2E6E","colab_type":"text"},"cell_type":"markdown","source":["## Text Prediction/Generation with Keras using <font color= #13c113  >LSTM: *Long Short Term Memory* networks</font>\n","\n","    In this example we will work with the book: Alice’s Adventures in Wonderland by Lewis Carroll.\n","\n","    We are going to learn the dependencies between characters and the conditional probabilities of characters in sequences so that we can in turn generate wholly new and original sequences of characters.\n","\n","<img src=https://3qeqpr26caki16dnhd19sv6by6v-wpengine.netdna-ssl.com/wp-content/uploads/2016/08/Text-Generation-With-LSTM-Recurrent-Neural-Networks-in-Python-with-Keras.jpg height=\"140\" width=\"200\">\n","\n","### Adapted from:\n","#### [Text Generation With LSTM Recurrent Neural Networks in Python with Keras](https://machinelearningmastery.com/text-generation-lstm-recurrent-neural-networks-python-keras/)\n","\n","By Jason Brownlee\n","\n","\n","<br>\n","\n","* <font size=5 color='green'>[MSTC](http://mstc.ssr.upm.es/big-data-track) seminar on Deep Learning, Tensorflow & Keras</font>"]},{"metadata":{"id":"d7BYb9WvopbE","colab_type":"text"},"cell_type":"markdown","source":["\n","\n","---\n","\n","## Start installing some libraries do some imports..."]},{"metadata":{"id":"r3E5B22Q_LuU","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"output_extras":[{"item_id":20}],"base_uri":"https://localhost:8080/","height":413},"outputId":"0bcaf556-8b54-4938-af9c-14af46aa0d87","executionInfo":{"status":"ok","timestamp":1521642143340,"user_tz":-60,"elapsed":19730,"user":{"displayName":"Luis Hernandez Gomez","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s128","userId":"114480253738065527469"}}},"cell_type":"code","source":["! pip install --upgrade keras"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Requirement already up-to-date: keras in /usr/local/lib/python3.6/dist-packages\n","Collecting pyyaml (from keras)\n","  Downloading PyYAML-3.12.tar.gz (253kB)\n","\u001b[K    100% |████████████████████████████████| 256kB 2.4MB/s \n","\u001b[?25hRequirement already up-to-date: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras)\n","Requirement already up-to-date: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras)\n","Collecting scipy>=0.14 (from keras)\n","  Downloading scipy-1.0.0-cp36-cp36m-manylinux1_x86_64.whl (50.0MB)\n","\u001b[K    100% |████████████████████████████████| 50.0MB 28kB/s \n","\u001b[?25hBuilding wheels for collected packages: pyyaml\n","  Running setup.py bdist_wheel for pyyaml ... \u001b[?25l-\b \b\\\b \bdone\n","\u001b[?25h  Stored in directory: /content/.cache/pip/wheels/2c/f7/79/13f3a12cd723892437c0cfbde1230ab4d82947ff7b3839a4fc\n","Successfully built pyyaml\n","Installing collected packages: pyyaml, scipy\n","  Found existing installation: PyYAML 3.11\n","    Uninstalling PyYAML-3.11:\n","      Successfully uninstalled PyYAML-3.11\n","  Found existing installation: scipy 0.19.1\n","    Uninstalling scipy-0.19.1:\n","      Successfully uninstalled scipy-0.19.1\n","Successfully installed pyyaml-3.12 scipy-1.0.0\n"],"name":"stdout"}]},{"metadata":{"id":"6UFqWyqn2E6P","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"output_extras":[{"item_id":1}],"base_uri":"https://localhost:8080/","height":36},"outputId":"c507dbd4-4b8e-4ad8-96db-1056e460e21e","executionInfo":{"status":"ok","timestamp":1521642147998,"user_tz":-60,"elapsed":4602,"user":{"displayName":"Luis Hernandez Gomez","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s128","userId":"114480253738065527469"}}},"cell_type":"code","source":["import numpy as np\n","from keras.models import Sequential\n","from keras.layers import Dense\n","from keras.layers import Dropout\n","from keras.layers import LSTM\n","from keras.callbacks import ModelCheckpoint\n","from keras.utils import np_utils\n"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"}]},{"metadata":{"id":"odDMNWMpVwpI","colab_type":"text"},"cell_type":"markdown","source":["\n","\n","---\n","\n","## Down load: <font color= #2a9dad >*Alice’s Adventures in Wonderland*</font>"]},{"metadata":{"id":"8BoeyLdlAMFk","colab_type":"text"},"cell_type":"markdown","source":["- ### We will first download the complete text in ASCII format (Plain Text UTF-8) \n","\n","- #### [Project Gutenberg](https://www.gutenberg.org/): gives free access to books that are no longer protected by copyright\n","\n","- ### Text has been prepared in a Google Drive link\n","\n"]},{"metadata":{"id":"wVLz9hSvmsiC","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"output_extras":[{"item_id":3}],"base_uri":"https://localhost:8080/","height":92},"outputId":"4f8e90f1-ed87-41c9-ebcf-f96c6d53829f","executionInfo":{"status":"ok","timestamp":1521642150578,"user_tz":-60,"elapsed":2496,"user":{"displayName":"Luis Hernandez Gomez","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s128","userId":"114480253738065527469"}}},"cell_type":"code","source":["! pip install googledrivedownloader"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Collecting googledrivedownloader\n","  Downloading googledrivedownloader-0.3-py2.py3-none-any.whl\n","Installing collected packages: googledrivedownloader\n","Successfully installed googledrivedownloader-0.3\n"],"name":"stdout"}]},{"metadata":{"id":"Db_o1gvImUm6","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"output_extras":[{"item_id":2}],"base_uri":"https://localhost:8080/","height":36},"outputId":"9c6e0572-c394-426f-9803-cb9df41d97f1","executionInfo":{"status":"ok","timestamp":1521642153700,"user_tz":-60,"elapsed":1260,"user":{"displayName":"Luis Hernandez Gomez","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s128","userId":"114480253738065527469"}}},"cell_type":"code","source":["from google_drive_downloader import GoogleDriveDownloader as gdd\n","\n","gdd.download_file_from_google_drive(file_id='1wG4PUnoYVUKrsaWgyWepSacUiYNNDEvM',\n","                                    dest_path='./wonderland.txt',\n","                                    unzip=False)"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Downloading 1wG4PUnoYVUKrsaWgyWepSacUiYNNDEvM into ./wonderland.txt... Done.\n"],"name":"stdout"}]},{"metadata":{"id":"jex6dqRlWsKQ","colab_type":"text"},"cell_type":"markdown","source":["- ### Read text for the book and convert all of the characters to lowercase to reduce the vocabulary that the network must learn"]},{"metadata":{"id":"R2IU6TLHWky6","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["# load ascii text and covert to lowercase\n","filename = \"wonderland.txt\"\n","raw_text = open(filename).read()\n","raw_text = raw_text.lower()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"e6_VWNBKa2ku","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"output_extras":[{"item_id":1}],"base_uri":"https://localhost:8080/","height":257},"outputId":"4efa2386-e211-41ce-ffa1-bdc5d59a0a7d","executionInfo":{"status":"ok","timestamp":1521394608558,"user_tz":-60,"elapsed":464,"user":{"displayName":"Luis Hernandez Gomez","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s128","userId":"114480253738065527469"}}},"cell_type":"code","source":["print(raw_text[0:200])\n"],"execution_count":86,"outputs":[{"output_type":"stream","text":["alice's adventures in wonderland\n","\n","lewis carroll\n","\n","the millennium fulcrum edition 3.0\n","\n","\n","\n","\n","chapter i. down the rabbit-hole\n","\n","alice was beginning to get very tired of sitting by her sister on the\n","bank, and\n"],"name":"stdout"}]},{"metadata":{"id":"J7ZPLsFQXGCE","colab_type":"text"},"cell_type":"markdown","source":["- ### We must use a \"numerical\" representation of text characters directly,\n","- ### We will start using a simple one: $integers$\n","- ### (Some characters could have been removed to further clean up the text)"]},{"metadata":{"id":"KbfoyekhuBtc","colab_type":"text"},"cell_type":"markdown","source":["\n","\n","---\n","\n","### We create a look-up tables $char\\_to\\_int$ and $int\\_to\\_char$ using Python dictionaries "]},{"metadata":{"id":"ByLYr2_5W5rG","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"output_extras":[{"item_id":1}],"base_uri":"https://localhost:8080/","height":55},"outputId":"eda242ea-6473-4ec4-eac8-1114c58448dc","executionInfo":{"status":"ok","timestamp":1521642180280,"user_tz":-60,"elapsed":542,"user":{"displayName":"Luis Hernandez Gomez","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s128","userId":"114480253738065527469"}}},"cell_type":"code","source":["# create mapping of unique chars to integers\n","chars = sorted(list(set(raw_text)))\n","char_to_int = dict((c, i) for i, c in enumerate(chars))\n","int_to_char = dict((i, c) for i, c in enumerate(chars))\n","\n","n_chars = len(raw_text)\n","n_vocab = len(chars)\n","print(\"Total Characters: \", n_chars)\n","print(\"Total Vocab: \", n_vocab)"],"execution_count":6,"outputs":[{"output_type":"stream","text":["Total Characters:  144431\n","Total Vocab:  45\n"],"name":"stdout"}]},{"metadata":{"id":"AWkj0YMVXjq4","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"output_extras":[{"item_id":1}],"base_uri":"https://localhost:8080/","height":35},"outputId":"8fe634e9-e45c-4ae3-a909-ea9500fa4ac9","executionInfo":{"status":"ok","timestamp":1521394609674,"user_tz":-60,"elapsed":470,"user":{"displayName":"Luis Hernandez Gomez","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s128","userId":"114480253738065527469"}}},"cell_type":"code","source":["print(chars[20:40])"],"execution_count":88,"outputs":[{"output_type":"stream","text":["['b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u']\n"],"name":"stdout"}]},{"metadata":{"id":"Bt8y8RI2t7c0","colab_type":"text"},"cell_type":"markdown","source":["---\n","\n","## Prediction Task:\n","\n","\n","- ### Number of steps: We will split the book text up into subsequences with a fixed length of 100 characters, an arbitrary length. \n","\n","- ### Each training batch of the network is comprised of 100 time steps of one character (X) followed by one character output (y). When creating these sequences, we slide this window along the whole book one character at a time, allowing each character a chance to be learned from the 100 characters that preceded it (except the first 100 characters of course).\n","\n"]},{"metadata":{"id":"ypvND-9uvG4g","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"output_extras":[{"item_id":2}],"base_uri":"https://localhost:8080/","height":55},"outputId":"30be79e8-8c56-44f7-e1be-a3795e8d7311","executionInfo":{"status":"ok","timestamp":1521642224870,"user_tz":-60,"elapsed":2806,"user":{"displayName":"Luis Hernandez Gomez","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s128","userId":"114480253738065527469"}}},"cell_type":"code","source":["# prepare the dataset of input to output pairs encoded as integers\n","seq_length = 100\n","\n","dataX = []\n","dataY = []\n","for i in range(0, n_chars - seq_length, 1):\n","\tseq_in = raw_text[i:i + seq_length]\n","\tseq_out = raw_text[i + seq_length]\n","\tdataX.append([char_to_int[char] for char in seq_in])\n","\tdataY.append(char_to_int[seq_out])\n","n_patterns = len(dataX)\n","\n","print(\"Total Patterns: \", n_patterns)\n","print(\"Pattern shape: \",np.array(dataX).shape)"],"execution_count":8,"outputs":[{"output_type":"stream","text":["Total Patterns:  144331\n","Pattern shape:  (144331, 100)\n"],"name":"stdout"}]},{"metadata":{"id":"gTPm6IwV0D7S","colab_type":"text"},"cell_type":"markdown","source":["- ### Let's see two examples:"]},{"metadata":{"id":"dNI1LWe8xOrI","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"output_extras":[{"item_id":1}],"base_uri":"https://localhost:8080/","height":187},"outputId":"02c31629-611f-4ffc-a643-3e6465c07cb9","executionInfo":{"status":"ok","timestamp":1521642227796,"user_tz":-60,"elapsed":556,"user":{"displayName":"Luis Hernandez Gomez","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s128","userId":"114480253738065527469"}}},"cell_type":"code","source":["print(\"\\\"\", ''.join([int_to_char[value] for value in dataX[201]]), \"\\\"\")\n","print(\"\\n\")\n","print(\"\\\"\", ''.join([int_to_char[value] for value in dataX[202]]), \"\\\"\")\n","print(\"\\n\")\n","print(\"\\\"\", ''.join([int_to_char[value] for value in dataY[201:216]]), \"\\\"\")"],"execution_count":9,"outputs":[{"output_type":"stream","text":["\" of having nothing to do: once or twice she had peeped into the\n","book her sister was reading, but it h \"\n","\n","\n","\" f having nothing to do: once or twice she had peeped into the\n","book her sister was reading, but it ha \"\n","\n","\n","\" ad no pictures  \"\n"],"name":"stdout"}]},{"metadata":{"id":"uDwrt7he0aIi","colab_type":"text"},"cell_type":"markdown","source":["\n","\n","---\n","\n","## We must now repare our training data to be suitable for use with LSTM in Keras.\n","\n","- ### First we must transform the list of input sequences into the form <font color= #3498db>  [no. samples or batches, time steps, features]</font> expected by an LSTM network.\n","\n","- ### Next we need to rescale the integers to the range 0-to-1 to make the patterns easier to learn by the LSTM network that uses the sigmoid activation function by default.\n","\n","- ### Finally, we need to convert the output patterns (single characters converted to integers) into a one hot encoding: to predict the probability of each of the different characters in the vocabulary"]},{"metadata":{"id":"Lk372vY7wLBw","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["# reshape X to be [samples, time steps, features]\n","X = np.reshape(dataX, (n_patterns, seq_length, 1))\n","# normalize\n","X = X / float(n_vocab)\n","# one hot encode the output variable\n","y = np_utils.to_categorical(dataY)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"vddjIgIp1_MM","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"output_extras":[{"item_id":1}],"base_uri":"https://localhost:8080/","height":92},"outputId":"312eab9e-0ac4-45c2-a970-763e865ca336","executionInfo":{"status":"ok","timestamp":1521642296872,"user_tz":-60,"elapsed":662,"user":{"displayName":"Luis Hernandez Gomez","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s128","userId":"114480253738065527469"}}},"cell_type":"code","source":["print(\"\\\"\", ''.join([int_to_char[int(value+0.5)] for value in X[200,:]*n_vocab]), \"\\\"\")\n","print(\"\\\"\", ''.join([int_to_char[int(value)] for value in dataX[200]]), \"\\\"\")"],"execution_count":11,"outputs":[{"output_type":"stream","text":["\"  of having nothing to do: once or twice she had peeped into the\n","book her sister was reading, but it  \"\n","\"  of having nothing to do: once or twice she had peeped into the\n","book her sister was reading, but it  \"\n"],"name":"stdout"}]},{"metadata":{"id":"F-4PJkMKL6qs","colab_type":"text"},"cell_type":"markdown","source":["---\n","## We can now define and compile our LSTM model:\n","- ### Here we define a single hidden LSTM layer with 256 memory units.\n","- ### The network uses dropout with a probability of 20.\n","- ### The output layer is a Dense layer using the softmax activation function to output a probability prediction for each of the possible characters between 0 and 1.\n","\n"]},{"metadata":{"id":"fs5ggo7a88ZK","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"output_extras":[{"item_id":1},{"item_id":2}],"base_uri":"https://localhost:8080/","height":111},"outputId":"01f7b543-76e2-43a9-a286-88ec0babccc1","executionInfo":{"status":"ok","timestamp":1521642455662,"user_tz":-60,"elapsed":534,"user":{"displayName":"Luis Hernandez Gomez","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s128","userId":"114480253738065527469"}}},"cell_type":"code","source":["print('X sjape: ', X.shape)\n","print('y.shape: ',y.shape)\n","y[0,:]"],"execution_count":16,"outputs":[{"output_type":"stream","text":["X sjape:  (144331, 100, 1)\n","y.shape:  (144331, 45)\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n","       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"]},"metadata":{"tags":[]},"execution_count":16}]},{"metadata":{"id":"9WThZ_vs2L_W","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"output_extras":[{"item_id":1}],"base_uri":"https://localhost:8080/","height":262},"outputId":"c724eb48-1860-453d-e814-5e293aa2e315","executionInfo":{"status":"ok","timestamp":1521642463872,"user_tz":-60,"elapsed":726,"user":{"displayName":"Luis Hernandez Gomez","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s128","userId":"114480253738065527469"}}},"cell_type":"code","source":["# define the LSTM model\n","model = Sequential()\n","model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2])))\n","model.add(Dropout(0.2))\n","model.add(Dense(y.shape[1], activation='softmax'))\n","model.compile(loss='categorical_crossentropy', optimizer='adam')\n","\n","model.summary()"],"execution_count":17,"outputs":[{"output_type":"stream","text":["_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","lstm_1 (LSTM)                (None, 256)               264192    \n","_________________________________________________________________\n","dropout_1 (Dropout)          (None, 256)               0         \n","_________________________________________________________________\n","dense_1 (Dense)              (None, 45)                11565     \n","=================================================================\n","Total params: 275,757\n","Trainable params: 275,757\n","Non-trainable params: 0\n","_________________________________________________________________\n"],"name":"stdout"}]},{"metadata":{"id":"LL5DlcUmNWpQ","colab_type":"text"},"cell_type":"markdown","source":["- ### Note that we not really are interested in prediction\n","- ### We are seeking a balance between generalization and overfitting but short of memorization.\n","- ### Because of the slowness of our optimization requirements, we will use model checkpointing to record all of the network weights to file each time an improvement in loss is observed at the end of the epoch.\n","- ### We will use the best set of weights (lowest loss) to instantiate our generative model in the next section."]},{"metadata":{"id":"HjQRFIXQMyzw","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["# define the checkpoint\n","filepath=\"weights-improvement-{epoch:02d}-{loss:.4f}.hdf5\"\n","checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n","callbacks_list = [checkpoint]"],"execution_count":0,"outputs":[]},{"metadata":{"id":"almMRAEOOQdq","colab_type":"text"},"cell_type":"markdown","source":["---\n","\n","## Fit our model to the data.\n","- ### Here we use a modest number of 20 epochs and a large batch size of 128 pattern"]},{"metadata":{"id":"PSHIipg-O8Cq","colab_type":"text"},"cell_type":"markdown","source":["\n","\n","---\n","\n","## Generating Text with an LSTM Network"]},{"metadata":{"id":"rvJOEfLBOgJ8","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"output_extras":[{"item_id":838},{"item_id":1685},{"item_id":2063},{"item_id":2064}],"base_uri":"https://localhost:8080/","height":1573},"outputId":"1116d7d3-1877-4300-e605-c1220a66d961","executionInfo":{"status":"error","timestamp":1521643149522,"user_tz":-60,"elapsed":551910,"user":{"displayName":"Luis Hernandez Gomez","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s128","userId":"114480253738065527469"}}},"cell_type":"code","source":["model.fit(X, y, epochs=20, batch_size=128, callbacks=callbacks_list)"],"execution_count":19,"outputs":[{"output_type":"stream","text":["Epoch 1/20\n","144331/144331 [==============================] - 225s 2ms/step - loss: 2.9559\n","\n","Epoch 00001: loss improved from inf to 2.95589, saving model to weights-improvement-01-2.9559.hdf5\n"],"name":"stdout"},{"output_type":"stream","text":["Epoch 2/20\n","144331/144331 [==============================] - 225s 2ms/step - loss: 2.7624\n"],"name":"stdout"},{"output_type":"stream","text":["\n","Epoch 00002: loss improved from 2.95589 to 2.76245, saving model to weights-improvement-02-2.7624.hdf5\n","Epoch 3/20\n"," 64256/144331 [============>.................] - ETA: 2:04 - loss: 2.6928"],"name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-19-2a61ae718648>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/models.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m    961\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    962\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 963\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m    964\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    965\u001b[0m     def evaluate(self, x=None, y=None,\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1703\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1704\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1705\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1706\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1707\u001b[0m     def evaluate(self, x=None, y=None,\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1233\u001b[0m                         \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1235\u001b[0;31m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1236\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1237\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2476\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2477\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2478\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2479\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    903\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    904\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 905\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    906\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1135\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1136\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1137\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1138\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1139\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1353\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1354\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1355\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1356\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1357\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1359\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1360\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1361\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1362\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1363\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1338\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1339\u001b[0m           return tf_session.TF_Run(session, options, feed_dict, fetch_list,\n\u001b[0;32m-> 1340\u001b[0;31m                                    target_list, status, run_metadata)\n\u001b[0m\u001b[1;32m   1341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1342\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"metadata":{"id":"dut9u1dCU4_8","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"output_extras":[{"item_id":1}],"base_uri":"https://localhost:8080/","height":109},"outputId":"60046fa5-c436-4753-d40f-4a3453f4e616","executionInfo":{"status":"ok","timestamp":1521396981818,"user_tz":-60,"elapsed":726,"user":{"displayName":"Luis Hernandez Gomez","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s128","userId":"114480253738065527469"}}},"cell_type":"code","source":["ls"],"execution_count":100,"outputs":[{"output_type":"stream","text":["\u001b[0m\u001b[01;34mdatalab\u001b[0m/                            weights-improvement-05-2.5211.hdf5\r\n","weights-improvement-01-2.9627.hdf5  weights-improvement-06-2.4663.hdf5\r\n","weights-improvement-02-2.7587.hdf5  weights-improvement-07-2.4137.hdf5\r\n","weights-improvement-03-2.6563.hdf5  weights-improvement-08-2.3627.hdf5\r\n","weights-improvement-04-2.5819.hdf5  wonderland.txt\r\n"],"name":"stdout"}]},{"metadata":{"id":"BgfgxAWcO0Jk","colab_type":"text"},"cell_type":"markdown","source":["\n","---\n","\n","## The network weights are loaded from a checkpoint file and the network does not need to be trained."]},{"metadata":{"id":"9ZLq6SOWPMTK","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":["# load the network weights\n","filename = \"weights-improvement-08-2.3627.hdf5\"\n","model.load_weights(filename)\n","model.compile(loss='categorical_crossentropy', optimizer='adam')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"zesHff1Q2E6e","colab_type":"text"},"cell_type":"markdown","source":["---\n","\n","## Finally: make predictions.\n","\n","- ### The simplest way is to first start with a seed sequence as input and predict the next character\n","- ### then update the seed sequence to add the predicted character on the end and trim off the first character.\n","- ### ...repeat this process to predict new characters (e.g. a sequence of 1,000 characters in length).\n"]},{"metadata":{"id":"NtHbjyJlQqbE","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"output_extras":[{"item_id":135},{"item_id":139}],"base_uri":"https://localhost:8080/","height":185},"outputId":"1aa5a9a1-42b7-4f5c-e764-5c37673aca2e","executionInfo":{"status":"ok","timestamp":1521397287258,"user_tz":-60,"elapsed":36538,"user":{"displayName":"Luis Hernandez Gomez","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s128","userId":"114480253738065527469"}}},"cell_type":"code","source":["import sys\n","\n","#pick a random seed\n","start = np.random.randint(0, len(dataX)-1)\n","pattern = dataX[start]\n","print(\"Seed:\")\n","print(\"\\\"\", ''.join([int_to_char[value] for value in pattern]), \"\\\"\")\n","print('\\n GENERATE: \\n')\n","# generate characters\n","for i in range(1000):\n","\tx = np.reshape(pattern, (1, len(pattern), 1))\n","\tx = x / float(n_vocab)\n","\tprediction = model.predict(x, verbose=0)\n","\tindex = np.argmax(prediction)\n","\tresult = int_to_char[index]\n","\tseq_in = [int_to_char[value] for value in pattern]\n","\tsys.stdout.write(result)\n","\tpattern.append(index)\n","\tpattern = pattern[1:len(pattern)]\n","print(\"\\nDone.\")"],"execution_count":109,"outputs":[{"output_type":"stream","text":["Seed:\n","\" urprised to find that she remained the same\n","size: to be sure, this generally happens when one eats c \"\n","\n"," GENERATE: \n","\n","n an all toe toeee to the sooee  and she was ao all tae so her aeain, and the sas so tee thee th the sas oo tae io a lore tf the tooee  and she was ao all tae so her aeain, and the sas so tee thee th the sas oo tae io a lore tf the tooee  and she was ao all tae so her aeain, and the sas so tee thee th the sas oo tae io a lore tf the tooee  and she was ao all tae so her aeain, and the sas so tee thee th the sas oo tae io a lore tf the tooee  and she was ao all tae so her aeain, and the sas so tee thee th the sas oo tae io a lore tf the tooee  and she was ao all tae so her aeain, and the sas so tee thee th the sas oo tae io a lore tf the tooee  and she was ao all tae so her aeain, and the sas so tee thee th the sas oo tae io a lore tf the tooee  and she was ao all tae so her aeain, and the sas so tee thee th the sas oo tae io a lore tf the tooee  and she was ao all tae so her aeain, and the sas so tee thee th the sas oo tae io a lore tf the tooee  and she was a"],"name":"stdout"},{"output_type":"stream","text":["o all tae so her aeain, and\n","Done.\n"],"name":"stdout"}]},{"metadata":{"id":"BjSJJA6OVnkK","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"output_extras":[{"item_id":1}],"base_uri":"https://localhost:8080/","height":35},"outputId":"39ddf872-44aa-4e56-ae85-051aeb9fd3b6","executionInfo":{"status":"ok","timestamp":1521397185250,"user_tz":-60,"elapsed":518,"user":{"displayName":"Luis Hernandez Gomez","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s128","userId":"114480253738065527469"}}},"cell_type":"code","source":["result"],"execution_count":108,"outputs":[{"output_type":"execute_result","data":{"text/plain":["' '"]},"metadata":{"tags":[]},"execution_count":108}]},{"metadata":{"id":"nO2Tbo852E6g","colab_type":"text"},"cell_type":"markdown","source":["# You can look for some ideas and improvements in:\n","\n","- ### [Learn about EMBEDDINGS](https://github.com/fchollet/deep-learning-with-python-notebooks/blob/master/6.1-using-word-embeddings.ipynb)\n","\n","- ### [text-generation-lstm-recurrent-neural-networks-python-keras](https://machinelearningmastery.com/text-generation-lstm-recurrent-neural-networks-python-keras/)\n","\n","- ### [Deepanway Ghosal](https://github.com/deepanwayx/char-and-word-rnn-keras)\n","\n"]}]}