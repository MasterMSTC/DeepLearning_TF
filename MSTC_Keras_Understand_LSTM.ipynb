{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MSTC_Keras_Understand_LSTM.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "RJR2Fgttvstk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Understand the Difference Between  <font color= #FF103  >Return Sequences</font> and  <font color= #FF103>Return States</font> for LSTMs in Keras\n",
        "\n",
        "## Following: [return-sequences-and-return-states-for-lstms-in-keras](https://machinelearningmastery.com/return-sequences-and-return-states-for-lstms-in-keras/)\n",
        "\n",
        "## By Jason Brownle\n",
        "\n",
        "\n",
        "![LSTM Book](https://3qeqpr26caki16dnhd19sv6by6v-wpengine.netdna-ssl.com/wp-content/uploads/2017/07/Long-Short-Term-Memory-Networks-With-Python.png)\n",
        "\n",
        "\n",
        "## [BOOK : LSTM with Python](https://machinelearningmastery.com/lstms-with-python/)\n",
        "\n",
        "\n",
        "\n",
        "<br>\n",
        "\n",
        "\n",
        "\n",
        "# * [MSTC](http://mstc.ssr.upm.es/big-data-track) and MUIT: <font size=5 color='green'>Deep Learning with Tensorflow & Keras</font>\n"
      ]
    },
    {
      "metadata": {
        "id": "6OsepRXawxK5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##  You will know:\n",
        "\n",
        "- That **return sequences** return the <font color=red>**hidden state**</font> output <font color=magenta>for **each** input time step</font>.\n",
        "- That **return state** returns the <font color=red>**hidden state**</font> output and <font color=red>**cell state**</font> <font color=magenta>for **the last input** time step**</font>.\n",
        "- That return sequences and return state can be used at the same time"
      ]
    },
    {
      "metadata": {
        "id": "KDxUz1Ykx2qw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "## Return Sequences\n",
        "\n",
        "    Each LSTM cell will output one hidden state h for each input."
      ]
    },
    {
      "metadata": {
        "id": "mkj66MWazMyu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "-  ### As an example: we will use an INPUT to a LSTM : series with 3 time steps (<font color=red> remember the input shape to a LSTM! </font>)"
      ]
    },
    {
      "metadata": {
        "id": "w58WcMeLzORR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from numpy import array\n",
        "\n",
        "# define input data\n",
        "data = array([0.1, 0.2, 0.3]).reshape((1,3,1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-u3CtVOP03lR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "-  ### We will define a single LSTM layer with a single unit: single hidden state and single cell\n",
        "- ### <font color=red> TO DO </font> Define the model using [Keras functinal API](https://keras.io/getting-started/functional-api-guide/)"
      ]
    },
    {
      "metadata": {
        "id": "GzKvfFNU1swx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from tensorflow import keras\n",
        "\n",
        "from keras.models import Model\n",
        "from keras.layers import Input\n",
        "from keras.layers import LSTM\n",
        "\n",
        "\n",
        "# define model\n",
        "inputs1 = Input(shape=(3, 1))\n",
        "lstm1 = LSTM(1)(inputs1)\n",
        "model = Model(inputs=inputs1, outputs=lstm1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "O_L2UXKR1Nuz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### NOTE: that we can use this model to predict **without** any training to predict (randomly initialized weights will be used)"
      ]
    },
    {
      "metadata": {
        "id": "L_DvXtXt2euB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "---\n",
        "**Running the example outputs a single hidden state for the input sequence with 3 time steps.**\n",
        "\n",
        "**Your specific output value will differ given the random initialization of the LSTM weights and cell state**\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "   "
      ]
    },
    {
      "metadata": {
        "id": "WsKFTdbI3LXh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# make and show prediction\n",
        "print(model.predict(data))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zKfPaRTy3Tx0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "---\n",
        "#  <font color=magenta>return_sequences</font> \n",
        "-  ### It is possible to access the hidden state output for each input time step.\n",
        "\n",
        "- ### This can be done by setting the <font color=magenta>return_sequences</font> attribute to <font color=gren>True</font> when defining the LSTM layer, as follows:"
      ]
    },
    {
      "metadata": {
        "id": "7wcntN3avYmH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from tensorflow import keras\n",
        "\n",
        "from keras.models import Model\n",
        "from keras.layers import Input\n",
        "from keras.layers import LSTM\n",
        "\n",
        "\n",
        "# define model\n",
        "inputs1 = Input(shape=(3, 1))\n",
        "lstm1 = LSTM(1, return_sequences=True)(inputs1)\n",
        "model = Model(inputs=inputs1, outputs=lstm1)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "41-0fnZv1Oaq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# make and show prediction\n",
        "print(model.predict(data))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xcdZgmJb4bfs",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "---\n",
        "### You must set return_sequences=True when\n",
        "\n",
        "- ### stacking LSTM layers so that the second LSTM layer has a three-dimensional sequence input. For more details, see the post:\n",
        "https://machinelearningmastery.com/stacked-long-short-term-memory-networks/ \n",
        "\n",
        "\n",
        "- ### When predicting a sequence of outputs with a Dense output layer wrapped in a <font color=red>TimeDistributed layer</font>. See this post for more details:\n",
        "https://machinelearningmastery.com/timedistributed-layer-for-long-short-term-memory-networks-in-python/\n",
        "\n",
        "![CNN + LSTM on Speech and Video for Emotion Recognition](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcSc8EDAzza1i2zmnT31TfLE6W0DJuUwKJCH4oryGNvAt1BNpohIPA)\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "AnU883wrPG8D",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "---\n",
        "#  <font color=magenta>return_states</font> \n",
        "\n",
        "\n",
        "- ## The output of an LSTM cell or layer of cells is called the <font color=red>hidden state</font>.\n",
        "\n",
        "- ## This is confusing, because each LSTM cell retains an internal state that is not output, called the <font color=red>cell state, or c.</font>\n",
        "\n",
        "---\n",
        "\n",
        "Generally, we do not need to access the cell state unless we are developing sophisticated models where subsequent layers may need to have their cell state initialized with the final cell state of another layer, such as in an encoder-decoder model.\n",
        "\n",
        "## Keras provides the <font color=magenta>return_state</font> argument to the LSTM layer that will provide access to the hidden state output (state_h) and the cell state (state_c).\n",
        "\n",
        "## For example:\n",
        "\n",
        " \t\n",
        " <font size=4>lstm1, state_h, state_c = LSTM(1, return_state=True)</font>\n",
        "\n",
        "**NOTE**:  <font size=4>This may look confusing because both <font color=brown>lstm1</font> and <font color=brown>state_h</font>  refer to the same hidden state output. The reason for these two tensors being separate will become clear in the next section.</font>"
      ]
    },
    {
      "metadata": {
        "id": "pFApoTtARusH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# define model\n",
        "inputs1 = Input(shape=(3, 1))\n",
        "lstm1, state_h, state_c = LSTM(1, return_state=True)(inputs1)\n",
        "model = Model(inputs=inputs1, outputs=[lstm1, state_h, state_c])\n",
        "\n",
        "# make and show prediction\n",
        "print(model.predict(data))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "D3TaU_Q8SZSp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "---\n",
        "#  <font color=magenta>Return States and Sequences</font> \n",
        "\n",
        "\n",
        "- ## We can access <font color=red>both</font> the sequence of <font color=red>hidden state</font> and the <font color=red>cell state</font> at the same time.\n",
        "\n",
        "- ## This can be done by configuring the LSTM layer to both return sequences and return states.\n",
        "\n",
        "\n",
        "<font size=4>lstm1, state_h, state_c = LSTM(1, return_sequences=True, return_state=True)</font>\n",
        "---\n"
      ]
    },
    {
      "metadata": {
        "id": "WWsz6jWbTc-0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# define model\n",
        "inputs1 = Input(shape=(3, 1))\n",
        "lstm1, state_h, state_c = LSTM(1, return_sequences=True , return_state=True)(inputs1)\n",
        "model = Model(inputs=inputs1, outputs=[lstm1, state_h, state_c])\n",
        "\n",
        "# make and show prediction\n",
        "print(model.predict(data))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IEfsLdmaT73F",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "- ## The layer returns the hidden state for each input time step, then separately, the hidden state output for the last time step and the cell state for the last input time step.\n",
        "\n",
        "- ## This can be confirmed by seeing that the last value in the returned sequences (first array) matches the value in the hidden state (second array)."
      ]
    }
  ]
}