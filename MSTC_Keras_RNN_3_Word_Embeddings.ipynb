{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"MSTC_Keras_RNN_3_Word_Embeddings.ipynb","version":"0.3.2","provenance":[{"file_id":"1ossNQSrXyvkmF7ZvZr7QPryk49A9tpCX","timestamp":1521206578358}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"metadata":{"id":"dUNKDytx2E6E","colab_type":"text"},"cell_type":"markdown","source":["# <font color= #13c113  >Word Embeddings</font> with Keras\n","\n","![Deep Learning with Python](https://images-na.ssl-images-amazon.com/images/I/41DWjHboiyL._SX258_BO1,204,203,200_.jpg)\n","\n","\n","## Adapted from:\n","\n","### [Section 6.1-using-word-embeddings](https://github.com/fchollet/deep-learning-with-python-notebooks/blob/master/6.1-using-word-embeddings.ipynb)\n","\n","## By François Chollet\n","\n","\n","<br>\n","\n","\n","# * [MSTC](http://mstc.ssr.upm.es/big-data-track) and MUIT: <font size=5 color='green'>Deep Learning with Tensorflow & Keras</font>"]},{"metadata":{"id":"d7BYb9WvopbE","colab_type":"text"},"cell_type":"markdown","source":["\n","\n","---\n","\n","## A popular and powerful way to associate a vector with a word is the use of dense \"word vectors\", also called \"word embeddings\".\n","\n","![Word embeddings](https://s3.amazonaws.com/book.keras.io/img/ch6/word_embeddings.png)\n","\n","from François Chollet book.keras.io\n","\n","- #### One-hot encoding are binary, sparse (mostly made of zeros) and very high-dimensional (same dimensionality as the number of words in the vocabulary), \"word embeddings\" are low-dimensional floating point vectors (i.e. \"dense\" vectors, as opposed to sparse vectors).\n","- #### Unlike one-hot encoding, word embeddings are learned from data.\n","\n","It is common to see word embeddings that are 256-dimensional, 512-dimensional, or 1024-dimensional when dealing with very large vocabularies. On the other hand, one-hot encoding words generally leads to vectors that are 20,000-dimensional or higher (capturing a vocabulary of 20,000 token in this case). So, word embeddings pack more information into far fewer dimensions."]},{"metadata":{"id":"wNYtq_MIa3t4","colab_type":"text"},"cell_type":"markdown","source":["\n","\n","---\n","\n","### There are two ways to obtain word embeddings:\n","\n","-    Learn word embeddings jointly with the main task you care about (e.g. document classification or sentiment prediction). In this setup, you would start with random word vectors, then **learn your word vectors in the same way that you learn the weights of a neural network**.\n","\n","<br>\n","-    Load into your model word embeddings that were pre-computed using a different machine learning task than the one you are trying to solve. These are called **\"pre-trained word embeddings\"**.\n"]},{"metadata":{"id":"HAQNnW4qbYkO","colab_type":"text"},"cell_type":"markdown","source":["\n","---\n","\n","## Learning word embeddings with the $Embedding$ layer\n","\n","- It is reasonable to learn a new embedding space with every new task.\n","- Thankfully, backpropagation makes this really easy,\n","- and Keras makes it even easier.\n","\n","      It's just about learning the weights of a layer: the Embedding layer."]},{"metadata":{"id":"WwlUcixycE8S","colab_type":"code","colab":{}},"cell_type":"code","source":["import tensorflow\n","from tensorflow import keras\n","\n","from keras.layers import Embedding\n","\n","# The Embedding layer takes at least two arguments:\n","# the number of possible tokens, here 10000 (1 + maximum word index (we will see it is 9999)),\n","# and the dimensionality of the embeddings, here 8.\n","embedding_layer = Embedding(10000, 8)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"ReoMuQACcZMi","colab_type":"text"},"cell_type":"markdown","source":["\n","---\n","\n","### The Embedding layer is best understood as a dictionary:\n","- It takes as input integers, it looks up these integers into an internal dictionary, and it returns the associated vectors.\n","\n","- The Embedding layer takes as input a 2D tensor of integers, of shape (samples, sequence_length), where each entry is a sequence of integers.\n","\n","- The Embedding layer returns a 3D floating point tensor, of shape (samples, sequence_length, embedding_dimensionality).\n","\n","\n"]},{"metadata":{"id":"o0xBrJjRdK7g","colab_type":"text"},"cell_type":"markdown","source":["\n","---\n","### Training the Embedding layer:\n","\n","- When you instantiate an Embedding layer, its weights (its internal dictionary of token vectors) are initially random, just like with any other layer.\n","\n","- During training, these word vectors will be gradually adjusted via backpropagation, structuring the space into something that the downstream model can exploit. \n","\n","- Once fully trained, your embedding space will show a lot of structure -- a kind of structure specialized for the specific problem you were training your model for."]},{"metadata":{"id":"HOPbrrI9dmmY","colab_type":"text"},"cell_type":"markdown","source":["\n","---\n","\n","\n","## See example using the IMDB movie review sentiment prediction.\n","\n","    \"IMDB dataset\", a set of 50,000 highly-polarized reviews from the Internet Movie Database. They are split into 25,000 reviews for training and 25,000 reviews for testing, each set consisting in 50% negative and 50% positive reviews.\n","\n","    Just like the MNIST dataset, the IMDB dataset comes packaged with Keras. It has already been preprocessed: the reviews (sequences of words) have been turned into sequences of integers, where each integer stands for a specific word in a dictionary.\n","\n","\n","Let's quickly prepare the data. \n","\n","- We will restrict the movie reviews to the top 10,000 most common words\n","- and cut the reviews after only 20 words. \n","\n","**Our network will simply learn 8-dimensional embeddings for each of the 10,000 words, turn the input integer sequences (2D integer tensor) into embedded sequences (3D float tensor), flatten the tensor to 2D, and train a single Dense layer on top for classification.**"]},{"metadata":{"id":"6UFqWyqn2E6P","colab_type":"code","colab":{}},"cell_type":"code","source":["from keras.datasets import imdb\n","from keras import preprocessing\n","\n","# Number of words to consider as features\n","max_features = 10000\n","# Cut texts after this number of words \n","# (among top max_features most common words)\n","maxlen = 20\n","\n","# Load the data as lists of integers.\n","(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n","\n","# This turns our lists of integers\n","# into a 2D integer tensor of shape `(samples, maxlen)`\n","x_train = preprocessing.sequence.pad_sequences(x_train, maxlen=maxlen)\n","x_test = preprocessing.sequence.pad_sequences(x_test, maxlen=maxlen)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"B6hsahVKIpcs","colab_type":"code","colab":{}},"cell_type":"code","source":["print('Train data shape:', x_train.shape)\n","print('Test  data shape:', x_test.shape)\n","\n","\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"TQCTbYZy4MqR","colab_type":"text"},"cell_type":"markdown","source":["## Check that the maximum word index is 10000"]},{"metadata":{"id":"z-e6J2QX3-bV","colab_type":"code","colab":{}},"cell_type":"code","source":["import numpy as np\n","np.max(x_train)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"5CC9su-NJYXQ","colab_type":"text"},"cell_type":"markdown","source":["---\n","## Model with a first layer Embeddings on the input sequence + Flatten + Dense\n","\n","- Define the model\n","- Compile it\n","- Fit train data & evaluate with test data"]},{"metadata":{"id":"vQxFCnnQJDNM","colab_type":"code","colab":{}},"cell_type":"code","source":["from keras.models import Sequential\n","from keras.layers import Flatten, Dense\n","\n","batch_size=32\n","\n","model = Sequential()\n","\n","#### NOTE that Embedding requires input_length #######################################\n","# We specify the maximum input length to our Embedding layer\n","# so we can later flatten the embedded inputs\n","# This argument is required if you are going to connect Flatten then Dense layers upstream\n","\n","model.add(Embedding(10000, 8, input_length=maxlen))\n","# After the Embedding layer, \n","# our activations have shape `(samples, maxlen, 8)`.\n","\n","# We flatten the 3D tensor of embeddings \n","# into a 2D tensor of shape `(samples, maxlen * 8)`\n","model.add(Flatten())\n","\n","# We add the classifier on top\n","model.add(Dense(1, activation='sigmoid'))\n","model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n","model.summary()\n","\n","history = model.fit(x_train, y_train,\n","                    epochs=10,\n","                    batch_size=batch_size,\n","                    validation_split=0.2)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"HjiE8uw-UyQk","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]},{"metadata":{"id":"AD9QRXwQTJug","colab_type":"code","colab":{}},"cell_type":"code","source":["score, acc = model.evaluate(x_test, y_test,\n","                            batch_size=batch_size)\n","print('Test score without RNN:', score)\n","print('Test accuracy without RNN:', acc)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"vzyofwbIWEQM","colab_type":"text"},"cell_type":"markdown","source":["---\n","## Now test a Model with a first layer Embeddings on every input + LSTM + Dense\n","\n","- Define the model\n","- Compile it\n","- Fit train data & evaluate with test data"]},{"metadata":{"id":"dUbnUh6BU0PG","colab_type":"code","colab":{}},"cell_type":"code","source":["from keras.models import Sequential\n","from keras.layers import Flatten, Dense, LSTM\n","\n","batch_size=32\n","\n","model = Sequential()\n","\n","model.add(Embedding(10000, 8))\n","model.add(LSTM(8, dropout=0.2, recurrent_dropout=0.2))\n","model.add(Dense(1, activation='sigmoid'))\n","\n","model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n","model.summary()\n","\n","history = model.fit(x_train, y_train,\n","                    epochs=10,\n","                    batch_size=batch_size,\n","                    validation_split=0.2)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Sjb9jSFqVat4","colab_type":"code","colab":{}},"cell_type":"code","source":["score, acc = model.evaluate(x_test, y_test,\n","                            batch_size=batch_size)\n","print('Test score with LSTM:', score)\n","print('Test accuracy with LSTM:', acc)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"abzpUABkLURI","colab_type":"text"},"cell_type":"markdown","source":["## See Pre-trained models\n","\n","https://nlp.stanford.edu/projects/glove/\n","\n","## See Embeddings projector\n","\n","http://projector.tensorflow.org/"]}]}