{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "WORKING_MSTC_Keras_Understand_LSTM.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "RJR2Fgttvstk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Understand the Difference Between  <font color= #FF103  >Return Sequences</font> and  <font color= #FF103>Return States</font> for LSTMs in Keras\n",
        "\n",
        "## Following: [return-sequences-and-return-states-for-lstms-in-keras](https://machinelearningmastery.com/return-sequences-and-return-states-for-lstms-in-keras/)\n",
        "\n",
        "## By Jason Brownle\n",
        "\n",
        "\n",
        "![LSTM Book](https://3qeqpr26caki16dnhd19sv6by6v-wpengine.netdna-ssl.com/wp-content/uploads/2017/07/Long-Short-Term-Memory-Networks-With-Python.png)\n",
        "\n",
        "\n",
        "## [BOOK : LSTM with Python](https://machinelearningmastery.com/lstms-with-python/)\n",
        "\n",
        "\n",
        "\n",
        "<br>\n",
        "\n",
        "\n",
        "\n",
        "# * [MSTC](http://mstc.ssr.upm.es/big-data-track) and MUIT: <font size=5 color='green'>Deep Learning with Tensorflow & Keras</font>\n"
      ]
    },
    {
      "metadata": {
        "id": "6OsepRXawxK5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##  You will know:\n",
        "\n",
        "- That **return sequences** return the <font color=red>**hidden state**</font> output <font color=magenta>for **each** input time step</font>.\n",
        "- That **return state** returns the <font color=red>**hidden state**</font> output and <font color=red>**cell state**</font> <font color=magenta>for **the last input** time step**</font>.\n",
        "- That return sequences and return state can be used at the same time"
      ]
    },
    {
      "metadata": {
        "id": "KDxUz1Ykx2qw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "## Return Sequences\n",
        "\n",
        "    Each LSTM cell will output one hidden state h for each input."
      ]
    },
    {
      "metadata": {
        "id": "mkj66MWazMyu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "-  ### As an example: we will use an INPUT to a LSTM : series with 3 time steps (<font color=red> remember the input shape to a LSTM! </font>)"
      ]
    },
    {
      "metadata": {
        "id": "w58WcMeLzORR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from numpy import array\n",
        "\n",
        "# define input data\n",
        "data = array([0.1, 0.2, 0.3])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EKm8E8lbUxnb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<font size=4 color=  #e11140>TO DO: reshape to use as input to LSTM </font>"
      ]
    },
    {
      "metadata": {
        "id": "z4wbSL6EUvge",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "data = "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-u3CtVOP03lR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Define a single LSTM layer with a single unit: single hidden state and single cell\n",
        "- ### <font size=4 color=  #e11140>TO DO:  Define the model using [Keras functinal API](https://keras.io/getting-started/functional-api-guide/)"
      ]
    },
    {
      "metadata": {
        "id": "GzKvfFNU1swx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from tensorflow import keras\n",
        "\n",
        "\n",
        "???\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "O_L2UXKR1Nuz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### NOTE: that we can use this model to predict **without** any training to predict (randomly initialized weights will be used)"
      ]
    },
    {
      "metadata": {
        "id": "L_DvXtXt2euB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "---\n",
        "**Running the example outputs a single hidden state for the input sequence with 3 time steps.**\n",
        "\n",
        "**Your specific output value will differ given the random initialization of the LSTM weights and cell state**\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "   "
      ]
    },
    {
      "metadata": {
        "id": "NftNZx7eV8Lq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "- ### <font size=4 color=  #e11140>TO DO:  Use the model to \"process\" (predict) data "
      ]
    },
    {
      "metadata": {
        "id": "WsKFTdbI3LXh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# make and show prediction using data\n",
        "???"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zKfPaRTy3Tx0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "---\n",
        "#  <font color=magenta>return_sequences</font> \n",
        "-  ### It is possible to access the hidden state output for each input time step.\n",
        "\n",
        "- ### This can be done by setting the <font color=magenta>return_sequences</font> attribute to <font color=gren>True</font> when defining the LSTM layer, as follows:"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "UV5mr3tiWhK_"
      },
      "cell_type": "markdown",
      "source": [
        "- ### <font size=4 color=  #e11140>TO DO:  Create the model and \"process\" (predict) data "
      ]
    },
    {
      "metadata": {
        "id": "7wcntN3avYmH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from tensorflow import keras\n",
        "\n",
        "???\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "41-0fnZv1Oaq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# make and show prediction\n",
        "???"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xcdZgmJb4bfs",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "---\n",
        "### You must set return_sequences=True when\n",
        "\n",
        "- ### stacking LSTM layers so that the second LSTM layer has a three-dimensional sequence input. For more details, see the post:\n",
        "https://machinelearningmastery.com/stacked-long-short-term-memory-networks/ \n",
        "\n",
        "\n",
        "- ### When predicting a sequence of outputs with a Dense output layer wrapped in a <font color=red>TimeDistributed layer</font>. See this post for more details:\n",
        "https://machinelearningmastery.com/timedistributed-layer-for-long-short-term-memory-networks-in-python/\n",
        "\n",
        "![CNN + LSTM on Speech and Video for Emotion Recognition](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcSc8EDAzza1i2zmnT31TfLE6W0DJuUwKJCH4oryGNvAt1BNpohIPA)\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "AnU883wrPG8D",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "---\n",
        "#  <font color=magenta>return_states</font> \n",
        "\n",
        "\n",
        "- ## The output of an LSTM cell or layer of cells is called the <font color=red>hidden state</font>.\n",
        "\n",
        "- ## This is confusing, because each LSTM cell retains an internal state that is not output, called the <font color=red>cell state, or c.</font>\n",
        "\n",
        "---\n",
        "\n",
        "Generally, we do not need to access the cell state unless we are developing sophisticated models where subsequent layers may need to have their cell state initialized with the final cell state of another layer, such as in an encoder-decoder model.\n",
        "\n",
        "## Keras provides the <font color=magenta>return_state</font> argument to the LSTM layer that will provide access to the hidden state output (state_h) and the cell state (state_c).\n",
        "\n",
        "## For example:\n",
        "\n",
        " \t\n",
        " <font size=4>lstm1, state_h, state_c = LSTM(1, return_state=True)</font>\n",
        "\n",
        "**NOTE**:  <font size=4>This may look confusing because both <font color=brown>lstm1</font> and <font color=brown>state_h</font>  refer to the same hidden state output. The reason for these two tensors being separate will become clear in the next section.</font>"
      ]
    },
    {
      "metadata": {
        "id": "iamLhgXjWq2I",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "- ### <font size=4 color=  #e11140>TO DO:  Create the model and \"process\" (predict) data"
      ]
    },
    {
      "metadata": {
        "id": "pFApoTtARusH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# define model\n",
        "???\n",
        "\n",
        "# make and show prediction\n",
        "???"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "D3TaU_Q8SZSp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "---\n",
        "#  <font color=magenta>Return States and Sequences</font> \n",
        "\n",
        "\n",
        "- ## We can access <font color=red>both</font> the sequence of <font color=red>hidden state</font> and the <font color=red>cell state</font> at the same time.\n",
        "\n",
        "- ## This can be done by configuring the LSTM layer to both return sequences and return states.\n",
        "\n",
        "\n",
        "<font size=4>lstm1, state_h, state_c = LSTM(1, return_sequences=True, return_state=True)</font>\n",
        "---\n"
      ]
    },
    {
      "metadata": {
        "id": "QZwiAzBXWsjP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "- ### <font size=4 color=  #e11140>TO DO:  Create the model and \"process\" (predict) data"
      ]
    },
    {
      "metadata": {
        "id": "WWsz6jWbTc-0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# define model\n",
        "???\n",
        "\n",
        "# make and show prediction\n",
        "???"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IEfsLdmaT73F",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "- ## The layer returns the hidden state for each input time step, then separately, the hidden state output for the last time step and the cell state for the last input time step.\n",
        "\n",
        "- ## This can be confirmed by seeing that the last value in the returned sequences (first array) matches the value in the hidden state (second array)."
      ]
    }
  ]
}