{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"MSTC_Keras_RNN_3_Word_Embeddings_2018.ipynb","version":"0.3.2","views":{},"default_view":{},"provenance":[{"file_id":"1ossNQSrXyvkmF7ZvZr7QPryk49A9tpCX","timestamp":1521206578358}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"metadata":{"id":"dUNKDytx2E6E","colab_type":"text"},"cell_type":"markdown","source":["## <font color= #13c113  >Word Embeddings</font> with Keras\n","\n","<img src=https://images-na.ssl-images-amazon.com/images/I/41DWjHboiyL._SX258_BO1,204,203,200_.jpg height=\"240\" width=\"200\">\n","\n","### Adapted from:\n","\n","#### [6.1-using-word-embeddings](https://github.com/fchollet/deep-learning-with-python-notebooks/blob/master/6.1-using-word-embeddings.ipynb)\n","\n","By François Chollet\n","\n","\n","<br>\n","\n","* <font size=5 color='green'>[MSTC](http://mstc.ssr.upm.es/big-data-track) seminar on Deep Learning, Tensorflow & Keras</font>"]},{"metadata":{"id":"d7BYb9WvopbE","colab_type":"text"},"cell_type":"markdown","source":["\n","\n","---\n","\n","## A popular and powerful way to associate a vector with a word is the use of dense \"word vectors\", also called \"word embeddings\".\n","\n","<img src=https://s3.amazonaws.com/book.keras.io/img/ch6/word_embeddings.png height=\"300\" width=\"400\">\n","from François Chollet book.keras.io\n","\n","- #### One-hot encoding are binary, sparse (mostly made of zeros) and very high-dimensional (same dimensionality as the number of words in the vocabulary), \"word embeddings\" are low-dimensional floating point vectors (i.e. \"dense\" vectors, as opposed to sparse vectors).\n","- #### Unlike one-hot encoding, word embeddings are learned from data.\n","\n","It is common to see word embeddings that are 256-dimensional, 512-dimensional, or 1024-dimensional when dealing with very large vocabularies. On the other hand, one-hot encoding words generally leads to vectors that are 20,000-dimensional or higher (capturing a vocabulary of 20,000 token in this case). So, word embeddings pack more information into far fewer dimensions."]},{"metadata":{"id":"wNYtq_MIa3t4","colab_type":"text"},"cell_type":"markdown","source":["\n","\n","---\n","\n","### There are two ways to obtain word embeddings:\n","\n","-    Learn word embeddings jointly with the main task you care about (e.g. document classification or sentiment prediction). In this setup, you would start with random word vectors, then **learn your word vectors in the same way that you learn the weights of a neural network**.\n","\n","<br>\n","-    Load into your model word embeddings that were pre-computed using a different machine learning task than the one you are trying to solve. These are called **\"pre-trained word embeddings\"**.\n"]},{"metadata":{"id":"HAQNnW4qbYkO","colab_type":"text"},"cell_type":"markdown","source":["\n","---\n","\n","## Learning word embeddings with the $Embedding$ layer\n","\n","- It is reasonable to learn a new embedding space with every new task.\n","- Thankfully, backpropagation makes this really easy,\n","- and Keras makes it even easier.\n","\n","      It's just about learning the weights of a layer: the Embedding layer."]},{"metadata":{"id":"r3E5B22Q_LuU","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"output_extras":[{"item_id":20}],"base_uri":"https://localhost:8080/","height":413},"outputId":"8bbfb438-745c-4788-e36e-d69b784d8ecd","executionInfo":{"status":"ok","timestamp":1521644324648,"user_tz":-60,"elapsed":54796,"user":{"displayName":"Luis Hernandez Gomez","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s128","userId":"114480253738065527469"}}},"cell_type":"code","source":["! pip install --upgrade keras"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Requirement already up-to-date: keras in /usr/local/lib/python3.6/dist-packages\r\n","Requirement already up-to-date: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras)\n","Collecting pyyaml (from keras)\n","  Downloading PyYAML-3.12.tar.gz (253kB)\n","\u001b[K    100% |████████████████████████████████| 256kB 2.5MB/s \n","\u001b[?25hRequirement already up-to-date: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras)\n","Collecting scipy>=0.14 (from keras)\n","  Downloading scipy-1.0.0-cp36-cp36m-manylinux1_x86_64.whl (50.0MB)\n","\u001b[K    100% |████████████████████████████████| 50.0MB 26kB/s \n","\u001b[?25hBuilding wheels for collected packages: pyyaml\n","  Running setup.py bdist_wheel for pyyaml ... \u001b[?25l-\b \b\\\b \bdone\n","\u001b[?25h  Stored in directory: /content/.cache/pip/wheels/2c/f7/79/13f3a12cd723892437c0cfbde1230ab4d82947ff7b3839a4fc\n","Successfully built pyyaml\n","Installing collected packages: pyyaml, scipy\n","  Found existing installation: PyYAML 3.11\n","    Uninstalling PyYAML-3.11:\n","      Successfully uninstalled PyYAML-3.11\n","  Found existing installation: scipy 0.19.1\n","    Uninstalling scipy-0.19.1:\n","      Successfully uninstalled scipy-0.19.1\n","Successfully installed pyyaml-3.12 scipy-1.0.0\n"],"name":"stdout"}]},{"metadata":{"id":"WwlUcixycE8S","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"output_extras":[{"item_id":1}],"base_uri":"https://localhost:8080/","height":36},"outputId":"7efa16cc-21d3-46d9-c357-bdf32fda1880","executionInfo":{"status":"ok","timestamp":1521644551840,"user_tz":-60,"elapsed":3350,"user":{"displayName":"Luis Hernandez Gomez","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s128","userId":"114480253738065527469"}}},"cell_type":"code","source":["from keras.layers import Embedding\n","\n","# The Embedding layer takes at least two arguments:\n","# the number of possible tokens, here 1000 (1 + maximum word index),\n","# and the dimensionality of the embeddings, here 64.\n","embedding_layer = Embedding(1000, 64)"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"}]},{"metadata":{"id":"ReoMuQACcZMi","colab_type":"text"},"cell_type":"markdown","source":["\n","---\n","\n","### The Embedding layer is best understood as a dictionary:\n","- It takes as input integers, it looks up these integers into an internal dictionary, and it returns the associated vectors.\n","\n","- The Embedding layer takes as input a 2D tensor of integers, of shape (samples, sequence_length), where each entry is a sequence of integers.\n","\n","- The Embedding layer returns a 3D floating point tensor, of shape (samples, sequence_length, embedding_dimensionality).\n","\n","\n"]},{"metadata":{"id":"o0xBrJjRdK7g","colab_type":"text"},"cell_type":"markdown","source":["\n","---\n","### Training the Embedding layer:\n","\n","- When you instantiate an Embedding layer, its weights (its internal dictionary of token vectors) are initially random, just like with any other layer.\n","\n","- During training, these word vectors will be gradually adjusted via backpropagation, structuring the space into something that the downstream model can exploit. \n","\n","- Once fully trained, your embedding space will show a lot of structure -- a kind of structure specialized for the specific problem you were training your model for."]},{"metadata":{"id":"HOPbrrI9dmmY","colab_type":"text"},"cell_type":"markdown","source":["\n","---\n","\n","\n","## See example using the IMDB movie review sentiment prediction.\n","\n","    \"IMDB dataset\", a set of 50,000 highly-polarized reviews from the Internet Movie Database. They are split into 25,000 reviews for training and 25,000 reviews for testing, each set consisting in 50% negative and 50% positive reviews.\n","\n","    Just like the MNIST dataset, the IMDB dataset comes packaged with Keras. It has already been preprocessed: the reviews (sequences of words) have been turned into sequences of integers, where each integer stands for a specific word in a dictionary.\n","\n","\n","Let's quickly prepare the data. \n","\n","- We will restrict the movie reviews to the top 10,000 most common words\n","- and cut the reviews after only 20 words. \n","\n","**Our network will simply learn 8-dimensional embeddings for each of the 10,000 words, turn the input integer sequences (2D integer tensor) into embedded sequences (3D float tensor), flatten the tensor to 2D, and train a single Dense layer on top for classification.**"]},{"metadata":{"id":"6UFqWyqn2E6P","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"output_extras":[{"item_id":4}],"base_uri":"https://localhost:8080/","height":55},"outputId":"42014a61-2010-42cb-af12-21ea0cde520b","executionInfo":{"status":"ok","timestamp":1521644560864,"user_tz":-60,"elapsed":6816,"user":{"displayName":"Luis Hernandez Gomez","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s128","userId":"114480253738065527469"}}},"cell_type":"code","source":["from keras.datasets import imdb\n","from keras import preprocessing\n","\n","# Number of words to consider as features\n","max_features = 10000\n","# Cut texts after this number of words \n","# (among top max_features most common words)\n","maxlen = 20\n","\n","# Load the data as lists of integers.\n","(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n","\n","# This turns our lists of integers\n","# into a 2D integer tensor of shape `(samples, maxlen)`\n","x_train = preprocessing.sequence.pad_sequences(x_train, maxlen=maxlen)\n","x_test = preprocessing.sequence.pad_sequences(x_test, maxlen=maxlen)"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Downloading data from https://s3.amazonaws.com/text-datasets/imdb.npz\n","17465344/17464789 [==============================] - 1s 0us/step\n"],"name":"stdout"}]},{"metadata":{"id":"B6hsahVKIpcs","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"output_extras":[{"item_id":1}],"base_uri":"https://localhost:8080/","height":55},"outputId":"5e5d8dba-b6b9-449c-c400-7a6ea7769c73","executionInfo":{"status":"ok","timestamp":1521644562804,"user_tz":-60,"elapsed":504,"user":{"displayName":"Luis Hernandez Gomez","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s128","userId":"114480253738065527469"}}},"cell_type":"code","source":["print('Train data shape:', x_train.shape)\n","print('Test  data shape:', x_test.shape)\n","\n","\n"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Train data shape: (25000, 20)\n","Test  data shape: (25000, 20)\n"],"name":"stdout"}]},{"metadata":{"id":"5CC9su-NJYXQ","colab_type":"text"},"cell_type":"markdown","source":["---\n","## Model with a first layer Embeddings on the input sequence + Flatten + Dense\n","\n","- Define the model\n","- Compile it\n","- Fit train data & evaluate with test data"]},{"metadata":{"id":"vQxFCnnQJDNM","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"output_extras":[{"item_id":21},{"item_id":59}],"base_uri":"https://localhost:8080/","height":678},"outputId":"4385ef1f-8da2-40b1-a771-98c37c6d9383","executionInfo":{"status":"ok","timestamp":1521644587224,"user_tz":-60,"elapsed":15538,"user":{"displayName":"Luis Hernandez Gomez","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s128","userId":"114480253738065527469"}}},"cell_type":"code","source":["from keras.models import Sequential\n","from keras.layers import Flatten, Dense\n","\n","batch_size=32\n","\n","model = Sequential()\n","\n","#### NOTE that Embedding requires input_length #######################################\n","# We specify the maximum input length to our Embedding layer\n","# so we can later flatten the embedded inputs\n","# This argument is required if you are going to connect Flatten then Dense layers upstream\n","\n","model.add(Embedding(10000, 8, input_length=maxlen))\n","# After the Embedding layer, \n","# our activations have shape `(samples, maxlen, 8)`.\n","\n","# We flatten the 3D tensor of embeddings \n","# into a 2D tensor of shape `(samples, maxlen * 8)`\n","model.add(Flatten())\n","\n","# We add the classifier on top\n","model.add(Dense(1, activation='sigmoid'))\n","model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n","model.summary()\n","\n","history = model.fit(x_train, y_train,\n","                    epochs=10,\n","                    batch_size=batch_size,\n","                    validation_split=0.2)"],"execution_count":5,"outputs":[{"output_type":"stream","text":["_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","embedding_2 (Embedding)      (None, 20, 8)             80000     \n","_________________________________________________________________\n","flatten_1 (Flatten)          (None, 160)               0         \n","_________________________________________________________________\n","dense_1 (Dense)              (None, 1)                 161       \n","=================================================================\n","Total params: 80,161\n","Trainable params: 80,161\n","Non-trainable params: 0\n","_________________________________________________________________\n","Train on 20000 samples, validate on 5000 samples\n","Epoch 1/10\n","20000/20000 [==============================] - 2s 76us/step - loss: 0.6759 - acc: 0.6047 - val_loss: 0.6398 - val_acc: 0.6806\n","Epoch 2/10\n","20000/20000 [==============================] - 2s 75us/step - loss: 0.5658 - acc: 0.7429 - val_loss: 0.5467 - val_acc: 0.7208\n","Epoch 3/10\n","20000/20000 [==============================] - 2s 77us/step - loss: 0.4752 - acc: 0.7809 - val_loss: 0.5113 - val_acc: 0.7380\n","Epoch 4/10\n"," 6176/20000 [========>.....................] - ETA: 0s - loss: 0.4230 - acc: 0.8135"],"name":"stdout"},{"output_type":"stream","text":["20000/20000 [==============================] - 1s 73us/step - loss: 0.4264 - acc: 0.8077 - val_loss: 0.5008 - val_acc: 0.7452\n","Epoch 5/10\n","20000/20000 [==============================] - 1s 72us/step - loss: 0.3931 - acc: 0.8254 - val_loss: 0.4981 - val_acc: 0.7544\n","Epoch 6/10\n","20000/20000 [==============================] - 1s 72us/step - loss: 0.3669 - acc: 0.8397 - val_loss: 0.5013 - val_acc: 0.7536\n","Epoch 7/10\n","20000/20000 [==============================] - 1s 74us/step - loss: 0.3436 - acc: 0.8534 - val_loss: 0.5051 - val_acc: 0.7520\n","Epoch 8/10\n","20000/20000 [==============================] - 1s 74us/step - loss: 0.3225 - acc: 0.8655 - val_loss: 0.5131 - val_acc: 0.7486\n","Epoch 9/10\n","20000/20000 [==============================] - 1s 74us/step - loss: 0.3024 - acc: 0.8763 - val_loss: 0.5212 - val_acc: 0.7490\n","Epoch 10/10\n","20000/20000 [==============================] - 1s 75us/step - loss: 0.2841 - acc: 0.8858 - val_loss: 0.5301 - val_acc: 0.7462\n"],"name":"stdout"}]},{"metadata":{"id":"HjiE8uw-UyQk","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0}}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]},{"metadata":{"id":"AD9QRXwQTJug","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"output_extras":[{"item_id":4}],"base_uri":"https://localhost:8080/","height":74},"outputId":"77c36f2b-4fab-41f6-8553-7846826bd776","executionInfo":{"status":"ok","timestamp":1521644593190,"user_tz":-60,"elapsed":1150,"user":{"displayName":"Luis Hernandez Gomez","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s128","userId":"114480253738065527469"}}},"cell_type":"code","source":["score, acc = model.evaluate(x_test, y_test,\n","                            batch_size=batch_size)\n","print('Test score without RNN:', score)\n","print('Test accuracy without RNN:', acc)"],"execution_count":6,"outputs":[{"output_type":"stream","text":["25000/25000 [==============================] - 1s 25us/step\n","Test score without RNN: 0.5212544250965119\n","Test accuracy without RNN: 0.75596\n"],"name":"stdout"}]},{"metadata":{"id":"vzyofwbIWEQM","colab_type":"text"},"cell_type":"markdown","source":["---\n","## Now test a Model with a first layer Embeddings on every input + LSTM + Dense\n","\n","- Define the model\n","- Compile it\n","- Fit train data & evaluate with test data"]},{"metadata":{"id":"dUbnUh6BU0PG","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"output_extras":[{"item_id":39},{"item_id":228},{"item_id":435},{"item_id":605},{"item_id":654},{"item_id":703},{"item_id":725}],"base_uri":"https://localhost:8080/","height":665},"outputId":"935360fc-927c-4940-d05c-ca023f3387cc","executionInfo":{"status":"ok","timestamp":1521481766196,"user_tz":-60,"elapsed":298624,"user":{"displayName":"Luis Hernandez Gomez","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s128","userId":"114480253738065527469"}}},"cell_type":"code","source":["from keras.models import Sequential\n","from keras.layers import Flatten, Dense, LSTM\n","\n","batch_size=32\n","\n","model = Sequential()\n","\n","model.add(Embedding(10000, 8))\n","model.add(LSTM(8, dropout=0.2, recurrent_dropout=0.2))\n","model.add(Dense(1, activation='sigmoid'))\n","\n","model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n","model.summary()\n","\n","history = model.fit(x_train, y_train,\n","                    epochs=10,\n","                    batch_size=batch_size,\n","                    validation_split=0.2)"],"execution_count":15,"outputs":[{"output_type":"stream","text":["_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","embedding_6 (Embedding)      (None, None, 8)           80000     \n","_________________________________________________________________\n","lstm_3 (LSTM)                (None, 8)                 544       \n","_________________________________________________________________\n","dense_4 (Dense)              (None, 1)                 9         \n","=================================================================\n","Total params: 80,553\n","Trainable params: 80,553\n","Non-trainable params: 0\n","_________________________________________________________________\n","Train on 20000 samples, validate on 5000 samples\n","Epoch 1/10\n"," 6624/20000 [========>.....................] - ETA: 20s - loss: 0.6845 - acc: 0.5791"],"name":"stdout"},{"output_type":"stream","text":["20000/20000 [==============================] - 30s 2ms/step - loss: 0.6225 - acc: 0.6589 - val_loss: 0.5401 - val_acc: 0.7300\n","Epoch 2/10\n","20000/20000 [==============================] - 30s 1ms/step - loss: 0.4886 - acc: 0.7665 - val_loss: 0.5014 - val_acc: 0.7480\n","Epoch 3/10\n"," 1216/20000 [>.............................] - ETA: 26s - loss: 0.4508 - acc: 0.8026"],"name":"stdout"},{"output_type":"stream","text":["20000/20000 [==============================] - 29s 1ms/step - loss: 0.4475 - acc: 0.7945 - val_loss: 0.4979 - val_acc: 0.7498\n","Epoch 4/10\n","20000/20000 [==============================] - 29s 1ms/step - loss: 0.4262 - acc: 0.8030 - val_loss: 0.4928 - val_acc: 0.7556\n","Epoch 5/10\n","   96/20000 [..............................] - ETA: 26s - loss: 0.3415 - acc: 0.8542"],"name":"stdout"},{"output_type":"stream","text":["20000/20000 [==============================] - 29s 1ms/step - loss: 0.4107 - acc: 0.8163 - val_loss: 0.4883 - val_acc: 0.7570\n","Epoch 6/10\n","19680/20000 [============================>.] - ETA: 0s - loss: 0.3969 - acc: 0.8220"],"name":"stdout"},{"output_type":"stream","text":["20000/20000 [==============================] - 29s 1ms/step - loss: 0.3968 - acc: 0.8221 - val_loss: 0.4981 - val_acc: 0.7502\n","Epoch 7/10\n","20000/20000 [==============================] - 29s 1ms/step - loss: 0.3919 - acc: 0.8265 - val_loss: 0.4924 - val_acc: 0.7564\n","Epoch 8/10\n"," 4448/20000 [=====>........................] - ETA: 21s - loss: 0.3794 - acc: 0.8269"],"name":"stdout"},{"output_type":"stream","text":["20000/20000 [==============================] - 29s 1ms/step - loss: 0.3873 - acc: 0.8273 - val_loss: 0.5005 - val_acc: 0.7554\n","Epoch 9/10\n","20000/20000 [==============================] - 29s 1ms/step - loss: 0.3780 - acc: 0.8326 - val_loss: 0.4948 - val_acc: 0.7550\n","Epoch 10/10\n","  992/20000 [>.............................] - ETA: 26s - loss: 0.3941 - acc: 0.8276"],"name":"stdout"},{"output_type":"stream","text":["20000/20000 [==============================] - 29s 1ms/step - loss: 0.3738 - acc: 0.8349 - val_loss: 0.5036 - val_acc: 0.7528\n"],"name":"stdout"}]},{"metadata":{"id":"Sjb9jSFqVat4","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"output_extras":[{"item_id":35}],"base_uri":"https://localhost:8080/","height":72},"outputId":"991df190-173e-47a6-f640-cffd889a1f14","executionInfo":{"status":"ok","timestamp":1521482142980,"user_tz":-60,"elapsed":9194,"user":{"displayName":"Luis Hernandez Gomez","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s128","userId":"114480253738065527469"}}},"cell_type":"code","source":["score, acc = model.evaluate(x_test, y_test,\n","                            batch_size=batch_size)\n","print('Test score with LSTM:', score)\n","print('Test accuracy with LSTM:', acc)"],"execution_count":16,"outputs":[{"output_type":"stream","text":["25000/25000 [==============================] - 9s 351us/step\n","Test score with LSTM: 0.4953634677314758\n","Test accuracy with LSTM: 0.76464\n"],"name":"stdout"}]},{"metadata":{"id":"abzpUABkLURI","colab_type":"text"},"cell_type":"markdown","source":["## See Pre-trained models\n","\n","https://nlp.stanford.edu/projects/glove/\n","\n","## See Embeddings projector\n","\n","http://projector.tensorflow.org/"]}]}