{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"MSTC_Keras_RNN_2_Text.ipynb","version":"0.3.2","provenance":[{"file_id":"1ossNQSrXyvkmF7ZvZr7QPryk49A9tpCX","timestamp":1521206578358}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"dUNKDytx2E6E","colab_type":"text"},"cell_type":"markdown","source":["## Text Prediction/Generation with Keras using <font color= #13c113  >LSTM: *Long Short Term Memory* networks</font>\n","\n","    In this example we will work with the book: Alice’s Adventures in Wonderland by Lewis Carroll.\n","\n","    We are going to learn the dependencies between characters and the conditional probabilities of characters in sequences so that we can in turn generate wholly new and original sequences of characters.\n","    \n","![Text-Generation-With-LSTM-Recurrent-Neural-Networks-in-Python-with-Keras](https://3qeqpr26caki16dnhd19sv6by6v-wpengine.netdna-ssl.com/wp-content/uploads/2016/08/Text-Generation-With-LSTM-Recurrent-Neural-Networks-in-Python-with-Keras.jpg)\n","\n","\n","### Adapted from:\n","#### [Text Generation With LSTM Recurrent Neural Networks in Python with Keras](https://machinelearningmastery.com/text-generation-lstm-recurrent-neural-networks-python-keras/)\n","\n","By Jason Brownlee\n","\n","\n","<br>\n","\n","\n","# * [MSTC](http://mstc.ssr.upm.es/big-data-track) and MUIT: <font size=5 color='green'>Deep Learning with Tensorflow & Keras</font>"]},{"metadata":{"id":"d7BYb9WvopbE","colab_type":"text"},"cell_type":"markdown","source":["\n","\n","---\n","\n","## Start installing some libraries do some imports..."]},{"metadata":{"id":"6UFqWyqn2E6P","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"outputId":"498d81eb-e39b-4442-fe92-f9f319a4ed26","executionInfo":{"status":"ok","timestamp":1542134263307,"user_tz":-60,"elapsed":1748,"user":{"displayName":"Luis Hernandez Gomez","photoUrl":"","userId":"05302408753550929701"}}},"cell_type":"code","source":["import numpy as np\n","\n","import tensorflow\n","from tensorflow import keras\n","\n","from keras.models import Sequential\n","from keras.layers import Dense\n","from keras.layers import Dropout\n","from keras.layers import LSTM\n","from keras.callbacks import ModelCheckpoint\n","from keras.utils import np_utils\n"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"}]},{"metadata":{"id":"odDMNWMpVwpI","colab_type":"text"},"cell_type":"markdown","source":["\n","\n","---\n","\n","## Down load: <font color= #2a9dad >*Alice’s Adventures in Wonderland*</font>"]},{"metadata":{"id":"8BoeyLdlAMFk","colab_type":"text"},"cell_type":"markdown","source":["- ### We will first download the complete text in ASCII format (Plain Text UTF-8) \n","\n","- #### [Project Gutenberg](https://www.gutenberg.org/): gives free access to books that are no longer protected by copyright\n","\n","- ### Text has been prepared in a Google Drive link\n","\n"]},{"metadata":{"id":"wVLz9hSvmsiC","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":110},"outputId":"e7e8bd1f-be44-4d84-969d-646bdc045230","executionInfo":{"status":"ok","timestamp":1542134402914,"user_tz":-60,"elapsed":3486,"user":{"displayName":"Luis Hernandez Gomez","photoUrl":"","userId":"05302408753550929701"}}},"cell_type":"code","source":["! pip install googledrivedownloader"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Collecting googledrivedownloader\n","  Downloading https://files.pythonhosted.org/packages/7e/41/d59b2a5fcc7afeb40f23091694bd6e6a63ad118c93f834353ee5100285d5/googledrivedownloader-0.3-py2.py3-none-any.whl\n","Installing collected packages: googledrivedownloader\n","Successfully installed googledrivedownloader-0.3\n"],"name":"stdout"}]},{"metadata":{"id":"Db_o1gvImUm6","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"outputId":"642f1d09-a5fd-4e4e-c86f-653fcc7471ff","executionInfo":{"status":"ok","timestamp":1542134407196,"user_tz":-60,"elapsed":1359,"user":{"displayName":"Luis Hernandez Gomez","photoUrl":"","userId":"05302408753550929701"}}},"cell_type":"code","source":["from google_drive_downloader import GoogleDriveDownloader as gdd\n","\n","gdd.download_file_from_google_drive(file_id='1wG4PUnoYVUKrsaWgyWepSacUiYNNDEvM',\n","                                    dest_path='./wonderland.txt',\n","                                    unzip=False)"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Downloading 1wG4PUnoYVUKrsaWgyWepSacUiYNNDEvM into ./wonderland.txt... Done.\n"],"name":"stdout"}]},{"metadata":{"id":"jex6dqRlWsKQ","colab_type":"text"},"cell_type":"markdown","source":["- ### Read text for the book and convert all of the characters to lowercase to reduce the vocabulary that the network must learn"]},{"metadata":{"id":"R2IU6TLHWky6","colab_type":"code","colab":{}},"cell_type":"code","source":["# load ascii text and covert to lowercase\n","filename = \"wonderland.txt\"\n","raw_text = open(filename).read()\n","raw_text = raw_text.lower()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"e6_VWNBKa2ku","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":254},"outputId":"557a38f7-a2bb-4ede-cae6-076808369dab","executionInfo":{"status":"ok","timestamp":1542134422364,"user_tz":-60,"elapsed":513,"user":{"displayName":"Luis Hernandez Gomez","photoUrl":"","userId":"05302408753550929701"}}},"cell_type":"code","source":["print(raw_text[0:200])\n"],"execution_count":5,"outputs":[{"output_type":"stream","text":["alice's adventures in wonderland\n","\n","lewis carroll\n","\n","the millennium fulcrum edition 3.0\n","\n","\n","\n","\n","chapter i. down the rabbit-hole\n","\n","alice was beginning to get very tired of sitting by her sister on the\n","bank, and\n"],"name":"stdout"}]},{"metadata":{"id":"J7ZPLsFQXGCE","colab_type":"text"},"cell_type":"markdown","source":["- ### We must use a \"numerical\" representation of text characters directly,\n","- ### We will start using a simple one: $integers$\n","- ### (Some characters could have been removed to further clean up the text)"]},{"metadata":{"id":"F-HDPTBJZuDP","colab_type":"text"},"cell_type":"markdown","source":["<font color=  #c5273a  face=\"times, serif\" size=5>============================================<br>\n","**TO DO:** how many different characters in raw_text?  store then ordered in a list</font>"]},{"metadata":{"id":"ss0TJIsyaKYg","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":73},"outputId":"8e6646f6-49b7-4d0c-ce76-fc870ae55fce","executionInfo":{"status":"ok","timestamp":1542135109792,"user_tz":-60,"elapsed":547,"user":{"displayName":"Luis Hernandez Gomez","photoUrl":"","userId":"05302408753550929701"}}},"cell_type":"code","source":["chars = sorted(list(set(raw_text)))\n","\n","print(chars)\n","print('Number of characters: ',len(chars))"],"execution_count":13,"outputs":[{"output_type":"stream","text":["['\\n', ' ', '!', '\"', \"'\", '(', ')', '*', ',', '-', '.', '0', '3', ':', ';', '?', '[', ']', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n","Number of characters:  45\n"],"name":"stdout"}]},{"metadata":{"id":"IojHeJjubjiy","colab_type":"text"},"cell_type":"markdown","source":["<font color=  #c5273a  face=\"times, serif\" size=5>============================================<br>\n","**TO DO:**  MAP each character to an integer using a *dictionary*  with key(char) : value(int) </font>"]},{"metadata":{"id":"7GbIOVn7et0v","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"outputId":"3c6ff1fa-4be2-48ee-fb89-5e0ae364df73","executionInfo":{"status":"ok","timestamp":1542136246683,"user_tz":-60,"elapsed":585,"user":{"displayName":"Luis Hernandez Gomez","photoUrl":"","userId":"05302408753550929701"}}},"cell_type":"code","source":["tmp=dict({'a':1 , 'b':2})\n","\n","tmp['b']"],"execution_count":39,"outputs":[{"output_type":"execute_result","data":{"text/plain":["2"]},"metadata":{"tags":[]},"execution_count":39}]},{"metadata":{"id":"MqWm0ZgCcHHi","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":55},"outputId":"0abafd73-fab2-4262-91dd-b5f14dde115f","executionInfo":{"status":"ok","timestamp":1542135972652,"user_tz":-60,"elapsed":540,"user":{"displayName":"Luis Hernandez Gomez","photoUrl":"","userId":"05302408753550929701"}}},"cell_type":"code","source":["# some examples\n","tmp=dict(enumerate(chars))\n","print('tmp:', tmp)"],"execution_count":31,"outputs":[{"output_type":"stream","text":["tmp: {0: '\\n', 1: ' ', 2: '!', 3: '\"', 4: \"'\", 5: '(', 6: ')', 7: '*', 8: ',', 9: '-', 10: '.', 11: '0', 12: '3', 13: ':', 14: ';', 15: '?', 16: '[', 17: ']', 18: '_', 19: 'a', 20: 'b', 21: 'c', 22: 'd', 23: 'e', 24: 'f', 25: 'g', 26: 'h', 27: 'i', 28: 'j', 29: 'k', 30: 'l', 31: 'm', 32: 'n', 33: 'o', 34: 'p', 35: 'q', 36: 'r', 37: 's', 38: 't', 39: 'u', 40: 'v', 41: 'w', 42: 'x', 43: 'y', 44: 'z'}\n"],"name":"stdout"}]},{"metadata":{"id":"IB6KaKkGd8BJ","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":836},"outputId":"12671d9c-467d-48ae-e689-1756ca984701","executionInfo":{"status":"ok","timestamp":1542136025956,"user_tz":-60,"elapsed":556,"user":{"displayName":"Luis Hernandez Gomez","photoUrl":"","userId":"05302408753550929701"}}},"cell_type":"code","source":["[(c,i) for i, c in enumerate(chars)]"],"execution_count":32,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('\\n', 0),\n"," (' ', 1),\n"," ('!', 2),\n"," ('\"', 3),\n"," (\"'\", 4),\n"," ('(', 5),\n"," (')', 6),\n"," ('*', 7),\n"," (',', 8),\n"," ('-', 9),\n"," ('.', 10),\n"," ('0', 11),\n"," ('3', 12),\n"," (':', 13),\n"," (';', 14),\n"," ('?', 15),\n"," ('[', 16),\n"," (']', 17),\n"," ('_', 18),\n"," ('a', 19),\n"," ('b', 20),\n"," ('c', 21),\n"," ('d', 22),\n"," ('e', 23),\n"," ('f', 24),\n"," ('g', 25),\n"," ('h', 26),\n"," ('i', 27),\n"," ('j', 28),\n"," ('k', 29),\n"," ('l', 30),\n"," ('m', 31),\n"," ('n', 32),\n"," ('o', 33),\n"," ('p', 34),\n"," ('q', 35),\n"," ('r', 36),\n"," ('s', 37),\n"," ('t', 38),\n"," ('u', 39),\n"," ('v', 40),\n"," ('w', 41),\n"," ('x', 42),\n"," ('y', 43),\n"," ('z', 44)]"]},"metadata":{"tags":[]},"execution_count":32}]},{"metadata":{"id":"z3LONt7Ub3zt","colab_type":"code","colab":{}},"cell_type":"code","source":["char_to_int = dict((c, i) for i, c in enumerate(chars))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"QG_-e37heJhl","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"outputId":"f5846420-e1f0-4df1-d472-9445ac0dd532","executionInfo":{"status":"ok","timestamp":1542136063949,"user_tz":-60,"elapsed":507,"user":{"displayName":"Luis Hernandez Gomez","photoUrl":"","userId":"05302408753550929701"}}},"cell_type":"code","source":["char_to_int['z']"],"execution_count":35,"outputs":[{"output_type":"execute_result","data":{"text/plain":["44"]},"metadata":{"tags":[]},"execution_count":35}]},{"metadata":{"colab_type":"text","id":"QGakftCBeVdO"},"cell_type":"markdown","source":["<font color=  #c5273a  face=\"times, serif\" size=5>============================================<br>\n","**TO DO:**  MAP \"back\" each integer using a *dictionary*  int: char </font>"]},{"metadata":{"id":"ByLYr2_5W5rG","colab_type":"code","colab":{}},"cell_type":"code","source":["int_to_char = dict((i, c) for i, c in enumerate(chars))\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"z1LxL5F9fWZo","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"outputId":"355942cf-ded2-4633-85cd-d6e81c31958e","executionInfo":{"status":"ok","timestamp":1542136367963,"user_tz":-60,"elapsed":581,"user":{"displayName":"Luis Hernandez Gomez","photoUrl":"","userId":"05302408753550929701"}}},"cell_type":"code","source":["int_to_char[44]"],"execution_count":42,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'z'"]},"metadata":{"tags":[]},"execution_count":42}]},{"metadata":{"id":"rggkUQX7fP6j","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":53},"outputId":"a437ba28-f3e5-4f8d-a2e6-e99adddfe439","executionInfo":{"status":"ok","timestamp":1542136375993,"user_tz":-60,"elapsed":577,"user":{"displayName":"Luis Hernandez Gomez","photoUrl":"","userId":"05302408753550929701"}}},"cell_type":"code","source":["n_chars = len(raw_text)\n","n_vocab = len(chars)\n","print(\"Total Characters: \", n_chars)\n","print(\"Total Vocab: \", n_vocab)"],"execution_count":43,"outputs":[{"output_type":"stream","text":["Total Characters:  144431\n","Total Vocab:  45\n"],"name":"stdout"}]},{"metadata":{"id":"Bt8y8RI2t7c0","colab_type":"text"},"cell_type":"markdown","source":["---\n","\n","## Prediction Task:\n","\n","\n","- ### <font color=red> Number of steps</font>: We will split the book text up into subsequences with a <font color=red>fixed length of 100 characters, an arbitrary length</font>. \n","\n","- ### To train the network we slide a windows of seq_length = 100 characters along the whole book\n","\n"]},{"metadata":{"colab_type":"text","id":"38qvEI_bhSaq"},"cell_type":"markdown","source":["<font color=  #c5273a  face=\"times, serif\" size=5>============================================<br>\n","**TO DO:**  slide a window extracting a sequence of seq_length = 100 characters along the book and store it in dataX : the input to the network</font>"]},{"metadata":{"id":"O25hAvcYhfii","colab_type":"code","colab":{}},"cell_type":"code","source":["seq_length = 100\n","\n","dataX=[]\n","\n","for i in range(0, n_chars - seq_length, 1):\n","  seq_in = raw_text[i:i+seq_length]\n","  dataX.append(seq_in)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"HeTJAgJzijQQ","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":235},"outputId":"73650cd9-16f3-4bd3-a0e5-cbf46408e942","executionInfo":{"status":"ok","timestamp":1542137302259,"user_tz":-60,"elapsed":760,"user":{"displayName":"Luis Hernandez Gomez","photoUrl":"","userId":"05302408753550929701"}}},"cell_type":"code","source":["print('dataX length: ',len(dataX))\n","print('dataX first training example: \\n',dataX[0])"],"execution_count":51,"outputs":[{"output_type":"stream","text":["dataX length:  144331\n","dataX first training example: \n"," alice's adventures in wonderland\n","\n","lewis carroll\n","\n","the millennium fulcrum edition 3.0\n","\n","\n","\n","\n","chapter i. d\n"],"name":"stdout"}]},{"metadata":{"colab_type":"text","id":"fpanFsYCjFWv"},"cell_type":"markdown","source":["<font color=  #c5273a  face=\"times, serif\" size=5>============================================<br>\n","**TO DO:**   dataX MUST be numeric!!! make changes using our $char\\_to\\_int$ dictionary</font>"]},{"metadata":{"id":"WDcMgk6RkUFC","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"outputId":"b7660bb4-9fe5-473f-bf57-e2cad0581157","executionInfo":{"status":"ok","timestamp":1542137669573,"user_tz":-60,"elapsed":547,"user":{"displayName":"Luis Hernandez Gomez","photoUrl":"","userId":"05302408753550929701"}}},"cell_type":"code","source":["raw_text[0:10]"],"execution_count":61,"outputs":[{"output_type":"execute_result","data":{"text/plain":["\"alice's ad\""]},"metadata":{"tags":[]},"execution_count":61}]},{"metadata":{"id":"0BquQXjhjdv-","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"outputId":"4dfbe51a-a642-44cf-f859-3a8febabb97a","executionInfo":{"status":"ok","timestamp":1542137647012,"user_tz":-60,"elapsed":508,"user":{"displayName":"Luis Hernandez Gomez","photoUrl":"","userId":"05302408753550929701"}}},"cell_type":"code","source":["[char_to_int[char] for char in raw_text[0:10]]"],"execution_count":60,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[19, 30, 27, 21, 23, 4, 37, 1, 19, 22]"]},"metadata":{"tags":[]},"execution_count":60}]},{"metadata":{"id":"KMlzbpf8jZcf","colab_type":"code","colab":{}},"cell_type":"code","source":["seq_length = 100\n","\n","dataX=[]\n","\n","for i in range(0, n_chars - seq_length, 1):\n","  seq_in = raw_text[i:i+seq_length]\n","  dataX.append([char_to_int[char] for char in seq_in])"],"execution_count":0,"outputs":[]},{"metadata":{"id":"GzXntZ6KkgTP","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":92},"outputId":"092396ba-84a9-484b-b87f-6e57ac1bbbda","executionInfo":{"status":"ok","timestamp":1542137774309,"user_tz":-60,"elapsed":515,"user":{"displayName":"Luis Hernandez Gomez","photoUrl":"","userId":"05302408753550929701"}}},"cell_type":"code","source":["print('dataX length: ',len(dataX))\n","print('dataX first training example: \\n',dataX[0])"],"execution_count":65,"outputs":[{"output_type":"stream","text":["dataX length:  144331\n","dataX first training example: \n"," [19, 30, 27, 21, 23, 4, 37, 1, 19, 22, 40, 23, 32, 38, 39, 36, 23, 37, 1, 27, 32, 1, 41, 33, 32, 22, 23, 36, 30, 19, 32, 22, 0, 0, 30, 23, 41, 27, 37, 1, 21, 19, 36, 36, 33, 30, 30, 0, 0, 38, 26, 23, 1, 31, 27, 30, 30, 23, 32, 32, 27, 39, 31, 1, 24, 39, 30, 21, 36, 39, 31, 1, 23, 22, 27, 38, 27, 33, 32, 1, 12, 10, 11, 0, 0, 0, 0, 0, 21, 26, 19, 34, 38, 23, 36, 1, 27, 10, 1, 22]\n"],"name":"stdout"}]},{"metadata":{"colab_type":"text","id":"T0_kYn0DlCUM"},"cell_type":"markdown","source":["<font color=  #c5273a  face=\"times, serif\" size=5>============================================<br>\n","**TO DO:**   Now we have to create the output for each 100 characters windows: the OUTPUT will be the next character, that is: we will train to predict the next character after \"seeing\" 100 previous characters. </font>\n","  \n","- ### So add to the for loop some code to store the \"next character\" for each window in dataY  :  again this MUST be numeric!!! so use our $char\\_to\\_int$ dictionary</font>"]},{"metadata":{"id":"ypvND-9uvG4g","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":53},"outputId":"c6e62cea-b10e-423f-dc19-bf7ccdf9dd72","executionInfo":{"status":"ok","timestamp":1542137605239,"user_tz":-60,"elapsed":3600,"user":{"displayName":"Luis Hernandez Gomez","photoUrl":"","userId":"05302408753550929701"}}},"cell_type":"code","source":["\n","seq_length = 100\n","\n","dataX = []\n","dataY = []\n","for i in range(0, n_chars - seq_length, 1):\n","  \n","\tseq_in = raw_text[i:i + seq_length]\n","\tseq_out = raw_text[i + seq_length]\n","\tdataX.append([char_to_int[char] for char in seq_in])\n","\tdataY.append(char_to_int[seq_out])\n","  "],"execution_count":58,"outputs":[{"output_type":"stream","text":["Total Patterns:  144331\n","Pattern shape:  (144331, 100)\n"],"name":"stdout"}]},{"metadata":{"id":"KqguLk9umLdU","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":53},"outputId":"280853b8-f790-4ce1-cabc-8573794a3ba6","executionInfo":{"status":"ok","timestamp":1542138156050,"user_tz":-60,"elapsed":1535,"user":{"displayName":"Luis Hernandez Gomez","photoUrl":"","userId":"05302408753550929701"}}},"cell_type":"code","source":["n_patterns = len(dataX)\n","\n","print(\"Total Patterns: \", n_patterns)\n","print(\"Pattern shape: \",np.array(dataX).shape)"],"execution_count":66,"outputs":[{"output_type":"stream","text":["Total Patterns:  144331\n","Pattern shape:  (144331, 100)\n"],"name":"stdout"}]},{"metadata":{"id":"gTPm6IwV0D7S","colab_type":"text"},"cell_type":"markdown","source":["- ### Let's see two examples:"]},{"metadata":{"id":"xgRUi6HVnrIe","colab_type":"code","colab":{}},"cell_type":"code","source":["[int_to_char[value] for value in dataX[202:203]]"],"execution_count":0,"outputs":[]},{"metadata":{"id":"dNI1LWe8xOrI","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":345},"outputId":"f6129c22-bc29-454d-96b2-a1fd9fa8fe49","executionInfo":{"status":"ok","timestamp":1542138816035,"user_tz":-60,"elapsed":780,"user":{"displayName":"Luis Hernandez Gomez","photoUrl":"","userId":"05302408753550929701"}}},"cell_type":"code","source":["print(\"------Window input dataX -------------------------------------\")\n","print(\"\\\"\", ''.join([int_to_char[value] for value in dataX[201]]), \"\\\"\")\n","print(\"\\n -----Character to predict dataY:\")\n","print(\"\\\"\", int_to_char[dataY[201]])\n","print(\"\\n\")\n","print(\"------Window input dataX -------------------------------------\")\n","print(\"\\\"\", ''.join([int_to_char[value] for value in dataX[202]]), \"\\\"\")\n","print(\"\\n -----Character to predict dataY:\")\n","print(\"\\\"\", int_to_char[dataY[202]])\n","print(\"\\n\")\n","print(\"\\\"\", ''.join([int_to_char[value] for value in dataY[100:216]]), \"\\\"\")"],"execution_count":85,"outputs":[{"output_type":"stream","text":["------Window input dataX -------------------------------------\n","\" of having nothing to do: once or twice she had peeped into the\n","book her sister was reading, but it h \"\n","\n"," -----Character to predict dataY:\n","\" a\n","\n","\n","------Window input dataX -------------------------------------\n","\" f having nothing to do: once or twice she had peeped into the\n","book her sister was reading, but it ha \"\n","\n"," -----Character to predict dataY:\n","\" d\n","\n","\n","\"  of having nothing to do: once or twice she had peeped into the\n","book her sister was reading, but it had no pictures  \"\n"],"name":"stdout"}]},{"metadata":{"id":"uDwrt7he0aIi","colab_type":"text"},"cell_type":"markdown","source":["\n","\n","---\n","\n","## We must now repare our training data to be suitable for use with LSTM in Keras.\n","\n","- ### First we must transform the list of input sequences into the form <font color= #3498db>  [no. samples or batches, time steps, features]</font> expected by an LSTM network. <font color=red> NOTE that our number of features is 1</font>\n","\n","- ### Next we need to rescale the integers to the range 0-to-1 to make the patterns easier to learn by the LSTM network that uses the sigmoid activation function by default.\n","\n","- ### Finally, we need to convert the output patterns (single characters converted to integers) into a one hot encoding: to predict the probability of each of the different characters in the vocabulary"]},{"metadata":{"id":"Vnokv8nkpChv","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"outputId":"38057dac-fe79-4fd1-9e94-fc7ff4ef47dc","executionInfo":{"status":"ok","timestamp":1542138957853,"user_tz":-60,"elapsed":1477,"user":{"displayName":"Luis Hernandez Gomez","photoUrl":"","userId":"05302408753550929701"}}},"cell_type":"code","source":["print('dataX shape', np.array(dataX).shape)"],"execution_count":89,"outputs":[{"output_type":"stream","text":["dataX shape (144331, 100)\n"],"name":"stdout"}]},{"metadata":{"id":"Lk372vY7wLBw","colab_type":"code","colab":{}},"cell_type":"code","source":["# reshape X to be [samples, time steps, features]\n","X = np.reshape(dataX, (n_patterns, seq_length, 1))\n","# normalize\n","X = X / float(n_vocab)\n","# one hot encode the output variable\n","y = np_utils.to_categorical(dataY)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"YxOMpiTMpp2P","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":90},"outputId":"b69fce01-fe57-4d1f-836b-1a6e350eccc0","executionInfo":{"status":"ok","timestamp":1542139208877,"user_tz":-60,"elapsed":554,"user":{"displayName":"Luis Hernandez Gomez","photoUrl":"","userId":"05302408753550929701"}}},"cell_type":"code","source":["# or with keras\n","ykeras=keras.utils.to_categorical(dataY)\n","\n","print('OHE example numpy',y[3])\n","print('OHE example keras',ykeras[3])\n"],"execution_count":96,"outputs":[{"output_type":"stream","text":["OHE example numpy [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n","OHE example keras [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n"," 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"],"name":"stdout"}]},{"metadata":{"id":"QsAh3_py0Ywy","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":207},"outputId":"52cf68db-02c5-41c5-f6b5-26b49598a019","executionInfo":{"status":"ok","timestamp":1521656924780,"user_tz":-60,"elapsed":704,"user":{"displayName":"Luis Hernandez Gomez","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s128","userId":"114480253738065527469"}}},"cell_type":"code","source":["X[200,0:10]*n_vocab"],"execution_count":15,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[ 1.],\n","       [33.],\n","       [24.],\n","       [ 1.],\n","       [26.],\n","       [19.],\n","       [40.],\n","       [27.],\n","       [32.],\n","       [25.]])"]},"metadata":{"tags":[]},"execution_count":15}]},{"metadata":{"id":"muyhcmH_qUfR","colab_type":"text"},"cell_type":"markdown","source":["## NOTE that to go now -after normaization- from int to chat we must multiply by n_vocab and round to integer "]},{"metadata":{"id":"vddjIgIp1_MM","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":93},"outputId":"3b39430d-e3cc-4d1f-bdf5-5097025bacad","executionInfo":{"status":"ok","timestamp":1521657007756,"user_tz":-60,"elapsed":536,"user":{"displayName":"Luis Hernandez Gomez","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s128","userId":"114480253738065527469"}}},"cell_type":"code","source":["print(\"\\\"\", ''.join([int_to_char[int(value+0.5)] for value in X[200,:]*n_vocab]), \"\\\"\")\n","print(\"\\\"\", ''.join([int_to_char[int(value)] for value in dataX[200]]), \"\\\"\")"],"execution_count":18,"outputs":[{"output_type":"stream","text":["\"  of having nothing to do: once or twice she had peeped into the\n","book her sister was reading, but it  \"\n","\"  of having nothing to do: once or twice she had peeped into the\n","book her sister was reading, but it  \"\n"],"name":"stdout"}]},{"metadata":{"id":"F-4PJkMKL6qs","colab_type":"text"},"cell_type":"markdown","source":["---\n","## We can now define and compile our LSTM model:\n","- ### Here we define a single hidden LSTM layer with 256 memory units.\n","- ### The network uses dropout with a probability of 20.\n","- ### The output layer is a Dense layer using the softmax activation function to output a probability prediction for each of the possible characters between 0 and 1.\n","\n"]},{"metadata":{"id":"fs5ggo7a88ZK","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":108},"outputId":"6f204f70-92a8-4633-b24d-f90963ca2ab6","executionInfo":{"status":"ok","timestamp":1542139344132,"user_tz":-60,"elapsed":512,"user":{"displayName":"Luis Hernandez Gomez","photoUrl":"","userId":"05302408753550929701"}}},"cell_type":"code","source":["print('X shape: ', X.shape)\n","print('y.shape: ',y.shape)\n","y[0,:]"],"execution_count":97,"outputs":[{"output_type":"stream","text":["X shape:  (144331, 100, 1)\n","y.shape:  (144331, 45)\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n","       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)"]},"metadata":{"tags":[]},"execution_count":97}]},{"metadata":{"colab_type":"text","id":"ZEj8EzCZq56I"},"cell_type":"markdown","source":["<font color=  #c5273a  face=\"times, serif\" size=5>============================================<br>\n","**TO DO:**   Define the LSTM model using Sequential style. </font>\n"," "]},{"metadata":{"id":"9WThZ_vs2L_W","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":254},"outputId":"891372a2-a8bb-47be-d454-1d0655619869","executionInfo":{"status":"ok","timestamp":1542139467727,"user_tz":-60,"elapsed":980,"user":{"displayName":"Luis Hernandez Gomez","photoUrl":"","userId":"05302408753550929701"}}},"cell_type":"code","source":["# define the LSTM model\n","model = Sequential()\n","model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2])))\n","model.add(Dropout(0.2))\n","model.add(Dense(y.shape[1], activation='softmax'))\n","\n","model.compile(loss='categorical_crossentropy', optimizer='adam')\n","\n","model.summary()"],"execution_count":98,"outputs":[{"output_type":"stream","text":["_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","lstm_1 (LSTM)                (None, 256)               264192    \n","_________________________________________________________________\n","dropout_1 (Dropout)          (None, 256)               0         \n","_________________________________________________________________\n","dense_1 (Dense)              (None, 45)                11565     \n","=================================================================\n","Total params: 275,757\n","Trainable params: 275,757\n","Non-trainable params: 0\n","_________________________________________________________________\n"],"name":"stdout"}]},{"metadata":{"id":"LL5DlcUmNWpQ","colab_type":"text"},"cell_type":"markdown","source":["- ### Note that we not really are interested in prediction\n","- ### We are seeking a balance between generalization and overfitting but short of memorization.\n","- ### Because of the slowness of our optimization requirements, we will use model checkpointing to record all of the network weights to file each time an improvement in loss is observed at the end of the epoch.\n","- ### We will use the best set of weights (lowest loss) to instantiate our generative model in the next section."]},{"metadata":{"id":"a9Y67_F_rn8C","colab_type":"text"},"cell_type":"markdown","source":["## <font color=orange> Take a look to hdf5 !!!</font>\n","\n","    HDF5 is a data model, library, and file format for storing and managing data. It supports an unlimited variety of datatypes, and is designed for flexible and efficient I/O and for high volume and complex data. HDF5 is portable and is extensible, allowing applications to evolve in their use of HDF5. The HDF5 Technology suite includes tools and applications for managing, manipulating, viewing, and analyzing data in the HDF5 format.\n","\n","## [HDF5 Web portal](https://portal.hdfgroup.org/display/HDF5/HDF5)"]},{"metadata":{"id":"HjQRFIXQMyzw","colab_type":"code","colab":{}},"cell_type":"code","source":["# define the checkpoint\n","filepath=\"weights-improvement-{epoch:02d}-{loss:.4f}.hdf5\"\n","checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n","callbacks_list = [checkpoint]"],"execution_count":0,"outputs":[]},{"metadata":{"id":"almMRAEOOQdq","colab_type":"text"},"cell_type":"markdown","source":["---\n","\n","## Fit our model to the data.\n","- ### Here we use a modest number of 20 epochs and a large batch size of 128 pattern"]},{"metadata":{"colab_type":"text","id":"PWBd0kFOspoJ"},"cell_type":"markdown","source":["<font color=  #c5273a  face=\"times, serif\" size=5>============================================<br>\n","**TO DO:**   Fit the model, first with 20 epochs batch_size=128 AND $callbacks$ !! </font>\n"," "]},{"metadata":{"id":"rvJOEfLBOgJ8","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":938},"outputId":"38c15386-1802-418b-ee38-2502520f7e91","executionInfo":{"status":"error","timestamp":1542140365863,"user_tz":-60,"elapsed":310317,"user":{"displayName":"Luis Hernandez Gomez","photoUrl":"","userId":"05302408753550929701"}}},"cell_type":"code","source":["model.fit(X, y, epochs=20, batch_size=128, callbacks=callbacks_list)"],"execution_count":102,"outputs":[{"output_type":"stream","text":["Epoch 1/20\n","144331/144331 [==============================] - 298s 2ms/step - loss: 2.8677\n","\n","Epoch 00001: loss improved from inf to 2.86774, saving model to weights-improvement-01-2.8677.hdf5\n","Epoch 2/20\n","  5504/144331 [>.............................] - ETA: 4:44 - loss: 2.7791"],"name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-102-2a61ae718648>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"metadata":{"id":"u1L1Isk0tAUs","colab_type":"text"},"cell_type":"markdown","source":["## check that callbacks has stored best models"]},{"metadata":{"id":"dut9u1dCU4_8","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"outputId":"ff9ceea9-1ff8-4ddb-9485-58a13cc25ca4","executionInfo":{"status":"ok","timestamp":1542140379635,"user_tz":-60,"elapsed":2065,"user":{"displayName":"Luis Hernandez Gomez","photoUrl":"","userId":"05302408753550929701"}}},"cell_type":"code","source":["ls"],"execution_count":103,"outputs":[{"output_type":"stream","text":["\u001b[0m\u001b[01;34msample_data\u001b[0m/  weights-improvement-01-2.8677.hdf5  wonderland.txt\n"],"name":"stdout"}]},{"metadata":{"id":"PSHIipg-O8Cq","colab_type":"text"},"cell_type":"markdown","source":["\n","\n","---\n","\n","## Generating Text with an LSTM Network"]},{"metadata":{"id":"BgfgxAWcO0Jk","colab_type":"text"},"cell_type":"markdown","source":["\n","---\n","\n","## The network weights are loaded from a checkpoint file and the network does not need to be trained."]},{"metadata":{"id":"9ZLq6SOWPMTK","colab_type":"code","colab":{}},"cell_type":"code","source":["# load the network weights\n","filename = \"weights-improvement-01-2.8677.hdf5\"\n","model.load_weights(filename)\n","model.compile(loss='categorical_crossentropy', optimizer='adam')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"zesHff1Q2E6e","colab_type":"text"},"cell_type":"markdown","source":["---\n","\n","## Finally: make predictions.\n","\n","- ### The simplest way is to first start with a seed sequence as input and predict the next character\n","- ### then update the seed sequence to add the predicted character on the end and trim off the first character.\n","- ### ...repeat this process to predict new characters (e.g. a sequence of 1,000 characters in length).\n"]},{"metadata":{"id":"NtHbjyJlQqbE","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1047},"outputId":"92479432-42bc-41bd-c8b6-78e06fb5acfc","executionInfo":{"status":"error","timestamp":1542141078811,"user_tz":-60,"elapsed":1365,"user":{"displayName":"Luis Hernandez Gomez","photoUrl":"","userId":"05302408753550929701"}}},"cell_type":"code","source":["import sys\n","\n","#pick a random seed\n","start = np.random.randint(0, len(dataX)-1)\n","pattern = dataX[start]\n","print(\"Seed:\")\n","print(\"\\\"\", ''.join([int_to_char[value] for value in pattern]), \"\\\"\")\n","print('\\n GENERATE: \\n')\n","\n","# generate characters\n","for i in range(500):\n","  x = np.reshape(pattern, (1, len(pattern), 1))\n","  x = x / float(n_vocab)\n","  prediction = model.predict(x, verbose=0)\n","  index = np.argmax(prediction)\n","  result = int_to_char[index]\n"," \n","  #print every ouput character\n","  sys.stdout.write(result)\n","  \n","  # add output char\n","  pattern.append(index)\n","  # remove first char\n","  pattern = pattern[1:len(pattern)]\n","  \n","print(\"\\nDone.\")"],"execution_count":123,"outputs":[{"output_type":"stream","text":["Seed:\n","\" far off to trouble\n","myself about you: you must manage the best way you can;--but i must be\n","kind to th \"\n","\n"," GENERATE: \n","\n","e  aa  a"],"name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-123-58eeb25e66cb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m   \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m   \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_vocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m   \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m   \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprediction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m   \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint_to_char\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m   1167\u001b[0m                                             \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1168\u001b[0m                                             \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1169\u001b[0;31m                                             steps=steps)\n\u001b[0m\u001b[1;32m   1170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1171\u001b[0m     def train_on_batch(self, x, y,\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mpredict_loop\u001b[0;34m(model, f, ins, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m    292\u001b[0m                 \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 294\u001b[0;31m             \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    295\u001b[0m             \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbatch_index\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"metadata":{"id":"BjSJJA6OVnkK","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"outputId":"39ddf872-44aa-4e56-ae85-051aeb9fd3b6","executionInfo":{"status":"ok","timestamp":1521397185250,"user_tz":-60,"elapsed":518,"user":{"displayName":"Luis Hernandez Gomez","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s128","userId":"114480253738065527469"}}},"cell_type":"code","source":["result"],"execution_count":108,"outputs":[{"output_type":"execute_result","data":{"text/plain":["' '"]},"metadata":{"tags":[]},"execution_count":108}]},{"metadata":{"id":"nO2Tbo852E6g","colab_type":"text"},"cell_type":"markdown","source":["# You can look for some ideas and improvements in:\n","\n","- ### [Learn about EMBEDDINGS](https://github.com/fchollet/deep-learning-with-python-notebooks/blob/master/6.1-using-word-embeddings.ipynb)\n","\n","- ### [text-generation-lstm-recurrent-neural-networks-python-keras](https://machinelearningmastery.com/text-generation-lstm-recurrent-neural-networks-python-keras/)\n","\n","- ### [Deepanway Ghosal](https://github.com/deepanwayx/char-and-word-rnn-keras)\n","\n"]}]}