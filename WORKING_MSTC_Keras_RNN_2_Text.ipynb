{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"WORKING_MSTC_Keras_RNN_2_Text.ipynb","version":"0.3.2","provenance":[{"file_id":"1ossNQSrXyvkmF7ZvZr7QPryk49A9tpCX","timestamp":1521206578358}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"dUNKDytx2E6E","colab_type":"text"},"cell_type":"markdown","source":["## Text Prediction/Generation with Keras using <font color= #13c113  >LSTM: *Long Short Term Memory* networks</font>\n","\n","    In this example we will work with the book: Alice’s Adventures in Wonderland by Lewis Carroll.\n","\n","    We are going to learn the dependencies between characters and the conditional probabilities of characters in sequences so that we can in turn generate wholly new and original sequences of characters.\n","    \n","![Text-Generation-With-LSTM-Recurrent-Neural-Networks-in-Python-with-Keras](https://3qeqpr26caki16dnhd19sv6by6v-wpengine.netdna-ssl.com/wp-content/uploads/2016/08/Text-Generation-With-LSTM-Recurrent-Neural-Networks-in-Python-with-Keras.jpg)\n","\n","\n","### Adapted from:\n","#### [Text Generation With LSTM Recurrent Neural Networks in Python with Keras](https://machinelearningmastery.com/text-generation-lstm-recurrent-neural-networks-python-keras/)\n","\n","By Jason Brownlee\n","\n","\n","<br>\n","\n","\n","# * [MSTC](http://mstc.ssr.upm.es/big-data-track) and MUIT: <font size=5 color='green'>Deep Learning with Tensorflow & Keras</font>"]},{"metadata":{"id":"d7BYb9WvopbE","colab_type":"text"},"cell_type":"markdown","source":["\n","\n","---\n","\n","## Start installing some libraries do some imports..."]},{"metadata":{"id":"6UFqWyqn2E6P","colab_type":"code","colab":{}},"cell_type":"code","source":["import numpy as np\n","\n","import tensorflow\n","from tensorflow import keras\n","\n","from keras.models import Sequential\n","from keras.layers import Dense\n","from keras.layers import Dropout\n","from keras.layers import LSTM\n","from keras.callbacks import ModelCheckpoint\n","from keras.utils import np_utils\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"odDMNWMpVwpI","colab_type":"text"},"cell_type":"markdown","source":["\n","\n","---\n","\n","## Down load: <font color= #2a9dad >*Alice’s Adventures in Wonderland*</font>"]},{"metadata":{"id":"8BoeyLdlAMFk","colab_type":"text"},"cell_type":"markdown","source":["- ### We will first download the complete text in ASCII format (Plain Text UTF-8) \n","\n","- #### [Project Gutenberg](https://www.gutenberg.org/): gives free access to books that are no longer protected by copyright\n","\n","- ### Text has been prepared in a Google Drive link\n","\n"]},{"metadata":{"id":"wVLz9hSvmsiC","colab_type":"code","colab":{}},"cell_type":"code","source":["! pip install googledrivedownloader"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Db_o1gvImUm6","colab_type":"code","colab":{}},"cell_type":"code","source":["from google_drive_downloader import GoogleDriveDownloader as gdd\n","\n","gdd.download_file_from_google_drive(file_id='1wG4PUnoYVUKrsaWgyWepSacUiYNNDEvM',\n","                                    dest_path='./wonderland.txt',\n","                                    unzip=False)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"jex6dqRlWsKQ","colab_type":"text"},"cell_type":"markdown","source":["- ### Read text for the book and convert all of the characters to lowercase to reduce the vocabulary that the network must learn"]},{"metadata":{"id":"R2IU6TLHWky6","colab_type":"code","colab":{}},"cell_type":"code","source":["# load ascii text and covert to lowercase\n","filename = \"wonderland.txt\"\n","raw_text = open(filename).read()\n","raw_text = raw_text.lower()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"e6_VWNBKa2ku","colab_type":"code","colab":{}},"cell_type":"code","source":["print(raw_text[0:200])\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"J7ZPLsFQXGCE","colab_type":"text"},"cell_type":"markdown","source":["- ### We must use a \"numerical\" representation of text characters directly,\n","- ### We will start using a simple one: $integers$\n","- ### (Some characters could have been removed to further clean up the text)"]},{"metadata":{"id":"F-HDPTBJZuDP","colab_type":"text"},"cell_type":"markdown","source":["<font color=  #c5273a  face=\"times, serif\" size=5>============================================<br>\n","**TO DO:** how many different characters in raw_text?  store then ordered in a list</font>"]},{"metadata":{"id":"ss0TJIsyaKYg","colab_type":"code","colab":{}},"cell_type":"code","source":["chars = ???\n","\n","print(chars)\n","print('Number of characters: ',len(chars))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"IojHeJjubjiy","colab_type":"text"},"cell_type":"markdown","source":["<font color=  #c5273a  face=\"times, serif\" size=5>============================================<br>\n","**TO DO:**  MAP each character to an integer using a *dictionary*  with key(char) : value(int) </font>"]},{"metadata":{"id":"z3LONt7Ub3zt","colab_type":"code","colab":{}},"cell_type":"code","source":["char_to_int = ???"],"execution_count":0,"outputs":[]},{"metadata":{"id":"QG_-e37heJhl","colab_type":"code","colab":{}},"cell_type":"code","source":["char_to_int['z']"],"execution_count":0,"outputs":[]},{"metadata":{"colab_type":"text","id":"QGakftCBeVdO"},"cell_type":"markdown","source":["<font color=  #c5273a  face=\"times, serif\" size=5>============================================<br>\n","**TO DO:**  MAP \"back\" each integer using a *dictionary*  int: char </font>"]},{"metadata":{"id":"ByLYr2_5W5rG","colab_type":"code","colab":{}},"cell_type":"code","source":["int_to_char = ???\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"z1LxL5F9fWZo","colab_type":"code","colab":{}},"cell_type":"code","source":["int_to_char[44]"],"execution_count":0,"outputs":[]},{"metadata":{"id":"rggkUQX7fP6j","colab_type":"code","colab":{}},"cell_type":"code","source":["n_chars = len(raw_text)\n","n_vocab = len(chars)\n","print(\"Total Characters: \", n_chars)\n","print(\"Total Vocab: \", n_vocab)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Bt8y8RI2t7c0","colab_type":"text"},"cell_type":"markdown","source":["---\n","\n","## Prediction Task:\n","\n","\n","- ### <font color=red> Number of steps</font>: We will split the book text up into subsequences with a <font color=red>fixed length of 100 characters, an arbitrary length</font>. \n","\n","- ### To train the network we slide a windows of seq_length = 100 characters along the whole book\n","\n"]},{"metadata":{"colab_type":"text","id":"38qvEI_bhSaq"},"cell_type":"markdown","source":["<font color=  #c5273a  face=\"times, serif\" size=5>============================================<br>\n","**TO DO:**  slide a window extracting a sequence of seq_length = 100 characters along the book and store it in dataX : the input to the network</font>"]},{"metadata":{"id":"O25hAvcYhfii","colab_type":"code","colab":{}},"cell_type":"code","source":["seq_length = 100\n","\n","dataX=[]\n","\n","for i in ???"],"execution_count":0,"outputs":[]},{"metadata":{"id":"HeTJAgJzijQQ","colab_type":"code","colab":{}},"cell_type":"code","source":["print('dataX length: ',len(dataX))\n","print('dataX first training example: \\n',dataX[0])"],"execution_count":0,"outputs":[]},{"metadata":{"colab_type":"text","id":"fpanFsYCjFWv"},"cell_type":"markdown","source":["<font color=  #c5273a  face=\"times, serif\" size=5>============================================<br>\n","**TO DO:**   dataX MUST be numeric!!! make changes using our $char\\_to\\_int$ dictionary</font>"]},{"metadata":{"id":"WDcMgk6RkUFC","colab_type":"code","colab":{}},"cell_type":"code","source":["raw_text[0:10]"],"execution_count":0,"outputs":[]},{"metadata":{"id":"0BquQXjhjdv-","colab_type":"code","colab":{}},"cell_type":"code","source":["[char_to_int[char] for char in raw_text[0:10]]"],"execution_count":0,"outputs":[]},{"metadata":{"id":"KMlzbpf8jZcf","colab_type":"code","colab":{}},"cell_type":"code","source":["seq_length = 100\n","\n","dataX=[]\n","\n","for i in range(0, n_chars - seq_length, 1):\n","  seq_in = raw_text[i:i+seq_length]\n","  dataX.append(???)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"GzXntZ6KkgTP","colab_type":"code","colab":{}},"cell_type":"code","source":["print('dataX length: ',len(dataX))\n","print('dataX first training example: \\n',dataX[0])"],"execution_count":0,"outputs":[]},{"metadata":{"colab_type":"text","id":"T0_kYn0DlCUM"},"cell_type":"markdown","source":["<font color=  #c5273a  face=\"times, serif\" size=5>============================================<br>\n","**TO DO:**   Now we have to create the output for each 100 characters windows: the OUTPUT will be the next character, that is: we will train to predict the next character after \"seeing\" 100 previous characters. </font>\n","  \n","- ### So add to the for loop some code to store the \"next character\" for each window in dataY  :  again this MUST be numeric!!! so use our $char\\_to\\_int$ dictionary</font>"]},{"metadata":{"id":"ypvND-9uvG4g","colab_type":"code","colab":{}},"cell_type":"code","source":["\n","seq_length = 100\n","\n","dataX = []\n","dataY = []\n","for i in range(0, n_chars - seq_length, 1):\n","  \n","\tseq_in = raw_text[i:i + seq_length]\n","\tseq_out = raw_text[???]\n","\tdataX.append([char_to_int[char] for char in seq_in])\n","\tdataY.append(???)\n","  "],"execution_count":0,"outputs":[]},{"metadata":{"id":"KqguLk9umLdU","colab_type":"code","colab":{}},"cell_type":"code","source":["n_patterns = len(dataX)\n","\n","print(\"Total Patterns: \", n_patterns)\n","print(\"Pattern shape: \",np.array(dataX).shape)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"gTPm6IwV0D7S","colab_type":"text"},"cell_type":"markdown","source":["- ### Let's see two examples:"]},{"metadata":{"id":"xgRUi6HVnrIe","colab_type":"code","colab":{}},"cell_type":"code","source":["[int_to_char[value] for value in dataX[202:203]]"],"execution_count":0,"outputs":[]},{"metadata":{"id":"dNI1LWe8xOrI","colab_type":"code","colab":{}},"cell_type":"code","source":["print(\"------Window input dataX -------------------------------------\")\n","print(\"\\\"\", ''.join([int_to_char[value] for value in dataX[201]]), \"\\\"\")\n","print(\"\\n -----Character to predict dataY:\")\n","print(\"\\\"\", int_to_char[dataY[201]])\n","print(\"\\n\")\n","print(\"------Window input dataX -------------------------------------\")\n","print(\"\\\"\", ''.join([int_to_char[value] for value in dataX[202]]), \"\\\"\")\n","print(\"\\n -----Character to predict dataY:\")\n","print(\"\\\"\", int_to_char[dataY[202]])\n","print(\"\\n\")\n","print(\"\\\"\", ''.join([int_to_char[value] for value in dataY[100:216]]), \"\\\"\")"],"execution_count":0,"outputs":[]},{"metadata":{"id":"uDwrt7he0aIi","colab_type":"text"},"cell_type":"markdown","source":["\n","\n","---\n","\n","## We must now repare our training data to be suitable for use with LSTM in Keras.\n","\n","- ### First we must transform the list of input sequences into the form <font color= #3498db>  [no. samples or batches, time steps, features]</font> expected by an LSTM network. <font color=red> NOTE that our number of features is 1</font>\n","\n","- ### Next we need to rescale the integers to the range 0-to-1 to make the patterns easier to learn by the LSTM network that uses the sigmoid activation function by default.\n","\n","- ### Finally, we need to convert the output patterns (single characters converted to integers) into a one hot encoding: to predict the probability of each of the different characters in the vocabulary"]},{"metadata":{"id":"Vnokv8nkpChv","colab_type":"code","colab":{}},"cell_type":"code","source":["print('dataX shape', np.array(dataX).shape)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Lk372vY7wLBw","colab_type":"code","colab":{}},"cell_type":"code","source":["# reshape X to be [samples, time steps, features]\n","X = np.reshape(dataX, (n_patterns, seq_length, 1))\n","# normalize\n","X = X / float(n_vocab)\n","# one hot encode the output variable\n","y = np_utils.to_categorical(dataY)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"YxOMpiTMpp2P","colab_type":"code","colab":{}},"cell_type":"code","source":["# or with keras\n","ykeras=keras.utils.to_categorical(dataY)\n","\n","print('OHE example numpy',y[3])\n","print('OHE example keras',ykeras[3])\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"QsAh3_py0Ywy","colab_type":"code","colab":{}},"cell_type":"code","source":["X[200,0:10]*n_vocab"],"execution_count":0,"outputs":[]},{"metadata":{"id":"muyhcmH_qUfR","colab_type":"text"},"cell_type":"markdown","source":["## NOTE that to go now -after normaization- from int to chat we must multiply by n_vocab and round to integer "]},{"metadata":{"id":"vddjIgIp1_MM","colab_type":"code","colab":{}},"cell_type":"code","source":["print(\"\\\"\", ''.join([int_to_char[int(value+0.5)] for value in X[200,:]*n_vocab]), \"\\\"\")\n","print(\"\\\"\", ''.join([int_to_char[int(value)] for value in dataX[200]]), \"\\\"\")"],"execution_count":0,"outputs":[]},{"metadata":{"id":"F-4PJkMKL6qs","colab_type":"text"},"cell_type":"markdown","source":["---\n","## We can now define and compile our LSTM model:\n","- ### Here we define a single hidden LSTM layer with 256 memory units.\n","- ### The network uses dropout with a probability of 20.\n","- ### The output layer is a Dense layer using the softmax activation function to output a probability prediction for each of the possible characters between 0 and 1.\n","\n"]},{"metadata":{"id":"fs5ggo7a88ZK","colab_type":"code","colab":{}},"cell_type":"code","source":["print('X shape: ', X.shape)\n","print('y.shape: ',y.shape)\n","y[0,:]"],"execution_count":0,"outputs":[]},{"metadata":{"colab_type":"text","id":"ZEj8EzCZq56I"},"cell_type":"markdown","source":["<font color=  #c5273a  face=\"times, serif\" size=5>============================================<br>\n","**TO DO:**   Define the LSTM model using Sequential style. </font>\n"," "]},{"metadata":{"id":"9WThZ_vs2L_W","colab_type":"code","colab":{}},"cell_type":"code","source":["# define the LSTM model\n","model = Sequential()\n","model.add(???)\n","model.add(???)\n","model.add(???))\n","\n","model.compile(loss='categorical_crossentropy', optimizer='adam')\n","\n","model.summary()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"LL5DlcUmNWpQ","colab_type":"text"},"cell_type":"markdown","source":["- ### Note that we not really are interested in prediction\n","- ### We are seeking a balance between generalization and overfitting but short of memorization.\n","- ### Because of the slowness of our optimization requirements, we will use model checkpointing to record all of the network weights to file each time an improvement in loss is observed at the end of the epoch.\n","- ### We will use the best set of weights (lowest loss) to instantiate our generative model in the next section."]},{"metadata":{"id":"a9Y67_F_rn8C","colab_type":"text"},"cell_type":"markdown","source":["## <font color=orange> Take a look to hdf5 !!!</font>\n","\n","    HDF5 is a data model, library, and file format for storing and managing data. It supports an unlimited variety of datatypes, and is designed for flexible and efficient I/O and for high volume and complex data. HDF5 is portable and is extensible, allowing applications to evolve in their use of HDF5. The HDF5 Technology suite includes tools and applications for managing, manipulating, viewing, and analyzing data in the HDF5 format.\n","\n","## [HDF5 Web portal](https://portal.hdfgroup.org/display/HDF5/HDF5)"]},{"metadata":{"id":"HjQRFIXQMyzw","colab_type":"code","colab":{}},"cell_type":"code","source":["# define the checkpoint\n","filepath=\"weights-improvement-{epoch:02d}-{loss:.4f}.hdf5\"\n","checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n","callbacks_list = [checkpoint]"],"execution_count":0,"outputs":[]},{"metadata":{"id":"almMRAEOOQdq","colab_type":"text"},"cell_type":"markdown","source":["---\n","\n","## Fit our model to the data.\n","- ### Here we use a modest number of 20 epochs and a large batch size of 128 pattern"]},{"metadata":{"colab_type":"text","id":"PWBd0kFOspoJ"},"cell_type":"markdown","source":["<font color=  #c5273a  face=\"times, serif\" size=5>============================================<br>\n","**TO DO:**   Fit the model, first with 20 epochs batch_size=128 AND $callbacks$ !! </font>\n"," "]},{"metadata":{"id":"rvJOEfLBOgJ8","colab_type":"code","colab":{}},"cell_type":"code","source":["model.fit(???)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"u1L1Isk0tAUs","colab_type":"text"},"cell_type":"markdown","source":["## check that callbacks has stored best models"]},{"metadata":{"id":"dut9u1dCU4_8","colab_type":"code","colab":{}},"cell_type":"code","source":["ls"],"execution_count":0,"outputs":[]},{"metadata":{"id":"PSHIipg-O8Cq","colab_type":"text"},"cell_type":"markdown","source":["\n","\n","---\n","\n","## Generating Text with an LSTM Network"]},{"metadata":{"id":"BgfgxAWcO0Jk","colab_type":"text"},"cell_type":"markdown","source":["\n","---\n","\n","## The network weights are loaded from a checkpoint file and the network does not need to be trained."]},{"metadata":{"id":"9ZLq6SOWPMTK","colab_type":"code","colab":{}},"cell_type":"code","source":["# load the network weights\n","filename = \"weights-improvement-01-2.8677.hdf5\"\n","model.load_weights(filename)\n","model.compile(loss='categorical_crossentropy', optimizer='adam')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"zesHff1Q2E6e","colab_type":"text"},"cell_type":"markdown","source":["---\n","\n","## Finally: make predictions.\n","\n","- ### The simplest way is to first start with a seed sequence as input and predict the next character\n","- ### then update the seed sequence to add the predicted character on the end and trim off the first character.\n","- ### ...repeat this process to predict new characters (e.g. a sequence of 1,000 characters in length).\n"]},{"metadata":{"id":"NtHbjyJlQqbE","colab_type":"code","colab":{}},"cell_type":"code","source":["import sys\n","\n","#pick a random seed\n","start = np.random.randint(0, len(dataX)-1)\n","pattern = dataX[start]\n","print(\"Seed:\")\n","print(\"\\\"\", ''.join([int_to_char[value] for value in pattern]), \"\\\"\")\n","print('\\n GENERATE: \\n')\n","\n","# generate characters\n","for i in range(500):\n","  x = np.reshape(pattern, (1, len(pattern), 1))\n","  x = x / float(n_vocab)\n","  prediction = model.predict(x, verbose=0)\n","  index = np.argmax(prediction)\n","  result = int_to_char[index]\n"," \n","  #print every ouput character\n","  sys.stdout.write(result)\n","  \n","  # add output char\n","  pattern.append(index)\n","  # remove first char\n","  pattern = pattern[1:len(pattern)]\n","  \n","print(\"\\nDone.\")"],"execution_count":0,"outputs":[]},{"metadata":{"id":"BjSJJA6OVnkK","colab_type":"code","colab":{}},"cell_type":"code","source":["result"],"execution_count":0,"outputs":[]},{"metadata":{"id":"nO2Tbo852E6g","colab_type":"text"},"cell_type":"markdown","source":["# You can look for some ideas and improvements in:\n","\n","- ### [Learn about EMBEDDINGS](https://github.com/fchollet/deep-learning-with-python-notebooks/blob/master/6.1-using-word-embeddings.ipynb)\n","\n","- ### [text-generation-lstm-recurrent-neural-networks-python-keras](https://machinelearningmastery.com/text-generation-lstm-recurrent-neural-networks-python-keras/)\n","\n","- ### [Deepanway Ghosal](https://github.com/deepanwayx/char-and-word-rnn-keras)\n","\n"]}]}